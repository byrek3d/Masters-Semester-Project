{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import glob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names={0:\"crisislex\",1:\"crisisnlp_crisismmd\",2:\"news_notNews\",3:\"Covid_WNUT\", 4:\"crisisnlp_Sandy_Japolin\", 5:\"crisisnlp_19Crisis\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset_cleaning/dataset1Cleaned.pkl\n",
      "./dataset_cleaning/dataset2Cleaned.pkl\n",
      "./dataset_cleaning/dataset3Cleaned.pkl\n",
      "./dataset_cleaning/dataset4Cleaned.pkl\n",
      "./dataset_cleaning/dataset5Cleaned.pkl\n",
      "./dataset_cleaning/dataset6Cleaned.pkl\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob(\"./dataset_cleaning/*.pkl\")\n",
    "all_files.sort()\n",
    "datasets = []\n",
    "\n",
    "for filename in all_files:\n",
    "    print(filename)\n",
    "    df = pd.read_pickle(filename)\n",
    "    datasets.append(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_processing(tweet):\n",
    "    \n",
    "    tweet= tweet.lower()\n",
    "    \n",
    "    #Removing hyperlinks from the tweet\n",
    "    tweet_no_links=re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    #Generating the list of words in the tweet (hashtags and other punctuations removed)\n",
    "    def form_sentence(tweet):\n",
    "        tweet_blob = TextBlob(tweet)\n",
    "        return ' '.join(tweet_blob.words)\n",
    "    new_tweet = form_sentence(tweet_no_links)\n",
    "    \n",
    "    #Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(tweet):\n",
    "        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    no_punc_tweet = no_user_alpha(new_tweet)\n",
    "    \n",
    "    #Normalizing the words in tweets \n",
    "    def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return \" \".join(normalized_tweet)\n",
    "    \n",
    "    \n",
    "    return normalization(no_punc_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Informativeness</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211040709124440064</td>\n",
       "      <td>0</td>\n",
       "      <td>#Intern #US #TATTOO #Wisconsin #Ohio #NC #PA #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210864180218167296</td>\n",
       "      <td>0</td>\n",
       "      <td>Get in on the fun every Thursday with the @csi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>211157222699433985</td>\n",
       "      <td>0</td>\n",
       "      <td>Welcome to our newest STUDENTathlete- Reagan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>211162553659830272</td>\n",
       "      <td>0</td>\n",
       "      <td>Denver Post: #Colorado governor signs bill cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>211216962162933761</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretty sure I'm going to live in Manitou Sprin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  Informativeness  \\\n",
       "0  211040709124440064                0   \n",
       "1  210864180218167296                0   \n",
       "2  211157222699433985                0   \n",
       "3  211162553659830272                0   \n",
       "4  211216962162933761                0   \n",
       "\n",
       "                                                text  \n",
       "0  #Intern #US #TATTOO #Wisconsin #Ohio #NC #PA #...  \n",
       "1  Get in on the fun every Thursday with the @csi...  \n",
       "2  Welcome to our newest STUDENTathlete- Reagan B...  \n",
       "3  Denver Post: #Colorado governor signs bill cre...  \n",
       "4  Pretty sure I'm going to live in Manitou Sprin...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 1\n",
      "Processing dataset 2\n",
      "Processing dataset 3\n",
      "Processing dataset 4\n",
      "Processing dataset 5\n",
      "Processing dataset 6\n"
     ]
    }
   ],
   "source": [
    "for i,ds in enumerate(datasets):\n",
    "    print(\"Processing dataset {}\".format(i+1))\n",
    "    ds['text_processed']=ds['text'].apply(text_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "authors=pd.read_csv('./dataset_cleaning/tj/parsed/tweet_metadata.csv')\n",
    "users=pd.read_csv('./dataset_cleaning/tj/parsed/twitter_user.csv')\n",
    "users.drop(columns=['created_at', 'lang', 'name', 'screen_name', 'location','access'], inplace=True)\n",
    "users.columns=['author_id', 'has_description', 'bio_has_url', 'followers_count', 'friends_count',\n",
    "       'favourites_count', 'listed_count', 'statuses_count', 'protected',\n",
    "       'verified', 'default_profile', 'default_profile_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 1\n",
      "Processing dataset 2\n",
      "Processing dataset 3\n",
      "Processing dataset 4\n",
      "Processing dataset 5\n",
      "Processing dataset 6\n"
     ]
    }
   ],
   "source": [
    "for i,ds in enumerate(datasets):\n",
    "    print(\"Processing dataset {}\".format(i+1))\n",
    "    ds = pd.merge(ds, authors, on='id')\n",
    "    ds.drop(ds.columns.difference(['id', 'Informativeness', 'text_processed', 'author_id', 'tweet_type', 'retweet_count', 'favorite_count']), 1, inplace=True)\n",
    "    \n",
    "    ds =pd.merge(ds, users, on='author_id')\n",
    "    \n",
    "    ds[\"has_description\"] = ds[\"has_description\"].apply(lambda x: 0 if x is np.nan else 1)\n",
    "    ds[\"tweet_type\"] = ds[\"tweet_type\"].apply(lambda x: 0 if x =='tweet' else 1)\n",
    "    ds[\"bio_has_url\"] = ds[\"bio_has_url\"].apply(lambda x: 0 if x is np.nan else 1)\n",
    "    ds = ds.drop(columns=['id', 'author_id'])\n",
    "    \n",
    "    datasets[i] = ds\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Informativeness', 'text_processed', 'tweet_type', 'retweet_count',\n",
       "       'favorite_count', 'has_description', 'bio_has_url', 'followers_count',\n",
       "       'friends_count', 'favourites_count', 'listed_count', 'statuses_count',\n",
       "       'protected', 'verified', 'default_profile', 'default_profile_image'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "def grid_search_text_and_numerical(model, parameters, X_train_text, X_train_numerical):\n",
    "    best_score=0\n",
    "    best_params={}\n",
    "    best_clf= None\n",
    "    best_vectorizer = None\n",
    "    best_tfIdfTransformer=None\n",
    "    for ngram_range in [(1, 1), (1, 2)]:\n",
    "        for use_tf_idf in [True, False]:\n",
    "            for normalization_norm in [\"max\"]:\n",
    "\n",
    "                print(\"Iteration with {}, {}, {}\".format(ngram_range, use_tf_idf, normalization_norm))\n",
    "                vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "                vectorizer.fit(X_train_text)\n",
    "                X_train_text_vectorized = vectorizer.transform(X_train_text)\n",
    "\n",
    "                tfidf_transformer = TfidfTransformer(use_idf = use_tf_idf)\n",
    "                tfidf_transformer.fit(X_train_text_vectorized)\n",
    "                X_train_text_tfidf = tfidf_transformer.transform(X_train_text_vectorized)\n",
    "\n",
    "\n",
    "                X_train_merged = hstack((X_train_text_tfidf,np.array(np.array(X_train_numerical).astype(np.float))))\n",
    "\n",
    "                X_train_merged = normalize(X_train_merged, norm=normalization_norm, axis=0)\n",
    "    #             X_train_merged = scale(X_train_merged, with_mean= False, axis = 0)\n",
    "\n",
    "                clf = GridSearchCV(model, parameters, scoring='roc_auc', n_jobs=-1, verbose=2)\n",
    "                clf.fit(X_train_merged, y_train)\n",
    "\n",
    "                if(best_score < clf.best_score_):\n",
    "                    best_score = clf.best_score_\n",
    "                    best_params = clf.best_params_\n",
    "                    best_params[\"ngram_range\"] = ngram_range\n",
    "                    best_params[\"use_tf_idf\"]= use_tf_idf\n",
    "                    best_params[\"normalization_norm\"] = normalization_norm\n",
    "                    best_clf=clf\n",
    "\n",
    "                    best_vectorizer = vectorizer\n",
    "                    best_tfIdfTransformer=tfidf_transformer\n",
    "                \n",
    "    return best_score, best_params, best_clf, best_vectorizer, best_tfIdfTransformer\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test_and_score(X_test_text, X_test_numerical, best_vecotorizer, best_tfIdfTransformer, best_params, best_clf):\n",
    "\n",
    "    X_test_text_vectorized = best_vecotorizer.transform(X_test_text)\n",
    "    X_test_text_tfidf = best_tfIdfTransformer.transform(X_test_text_vectorized)\n",
    "\n",
    "\n",
    "    X_test_merged = hstack((X_test_text_tfidf,np.array(np.array(X_test_numerical).astype(np.float))))\n",
    "    \n",
    "    X_test_merged = normalize(X_test_merged, norm=best_params[\"normalization_norm\"], axis=0)\n",
    "    # X_test_merged = scale(X_test_merged, with_mean= False, axis = 0)\n",
    "\n",
    "    return best_clf.score(X_test_merged, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier( penalty='l2', random_state=42))])\n",
    "\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf-svm__alpha': (1e-5, 1e-3),\n",
    "                  'clf-svm__loss':('hinge','log','perceptron'),'clf-svm__max_iter': (10, 100)}\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm,scoring='roc_auc', n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_clf_lr = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-lr', LogisticRegression( max_iter=1000))])\n",
    "\n",
    "parameters_lr = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf-lr__penalty': ('l2', 'l1')}\n",
    "\n",
    "gs_clf_lr = GridSearchCV(text_clf_lr, parameters_lr ,scoring='roc_auc', n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "text_clf_nb = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-nb', MultinomialNB())])\n",
    "\n",
    "parameters_nb = {'vect__ngram_range': [(1, 1), (1, 2),(2,2)], 'tfidf__use_idf': (True, False),\n",
    "                  'clf-nb__alpha': (1,1e-1, 1e-3)}\n",
    "\n",
    "gs_clf_nb = GridSearchCV(text_clf_nb, parameters_nb,scoring='roc_auc', n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "text_clf_dt = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-dt', DecisionTreeClassifier(  ))])\n",
    "\n",
    "parameters_dt = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, ),\n",
    "                'clf-dt__max_depth':[200, 400, 1000]}\n",
    "\n",
    "gs_clf_dt = GridSearchCV(text_clf_dt, parameters_dt,scoring='roc_auc', n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "text_clf_rf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-rf', RandomForestClassifier( ))])\n",
    "\n",
    "parameters_rf = {'vect__ngram_range': [(1, 1),(2,2)], 'tfidf__use_idf': (True, False),\n",
    "                'clf-rf__n_estimators':[800,], 'clf-rf__min_samples_split':[4,10]}\n",
    "\n",
    "gs_clf_rf = GridSearchCV(text_clf_rf, parameters_rf, scoring='roc_auc', n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [gs_clf_svm, gs_clf_lr, gs_clf_nb, gs_clf_dt, gs_clf_rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_parameters = {'penalty':('l2',)}\n",
    "lr_model = LogisticRegression( max_iter=1000)\n",
    "\n",
    "lr={}\n",
    "lr[\"model\"]=lr_model\n",
    "lr[\"parameters\"] =lr_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svd_parameters = { 'loss':('hinge','log',),'max_iter': (100,1000),\n",
    "                 'alpha':(1e-3,1e-6, 1e-10),'penalty':('l2',)}\n",
    "svd_model = SGDClassifier( random_state=42, early_stopping=True)\n",
    "\n",
    "svd={}\n",
    "svd[\"model\"]=svd_model\n",
    "svd[\"parameters\"] =svd_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_parameters = { 'alpha': (1,1e-1, 1e-3)}\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb = {}\n",
    "nb[\"model\"]=nb_model\n",
    "nb[\"parameters\"] =nb_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_parameters = {'max_depth':[200, 400, 1000]}\n",
    "dt_model = DecisionTreeClassifier(  )\n",
    "\n",
    "dt={}\n",
    "dt[\"model\"]=dt_model\n",
    "dt[\"parameters\"] =dt_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_parameters = {'n_estimators':[800,], 'min_samples_split':[4,10]}\n",
    "rf_model = RandomForestClassifier( )\n",
    "\n",
    "rf={}\n",
    "rf[\"model\"]=rf_model\n",
    "rf[\"parameters\"] =rf_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [rf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Algo: {'model': RandomForestClassifier(), 'parameters': {'n_estimators': [800], 'min_samples_split': [4, 10]}}\n",
      "Training on Dataset 1\n",
      "Iteration with (1, 1), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 10.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 10.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 39.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 39.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on Dataset 1\n",
      "testing on Dataset 2\n",
      "testing on Dataset 3\n",
      "testing on Dataset 4\n",
      "testing on Dataset 5\n",
      "testing on Dataset 6\n",
      "Training on Dataset 2\n",
      "Iteration with (1, 1), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  7.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 22.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 22.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on Dataset 1\n",
      "testing on Dataset 2\n",
      "testing on Dataset 3\n",
      "testing on Dataset 4\n",
      "testing on Dataset 5\n",
      "testing on Dataset 6\n",
      "Training on Dataset 3\n",
      "Iteration with (1, 1), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   43.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   42.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on Dataset 1\n",
      "testing on Dataset 2\n",
      "testing on Dataset 3\n",
      "testing on Dataset 4\n",
      "testing on Dataset 5\n",
      "testing on Dataset 6\n",
      "Training on Dataset 4\n",
      "Iteration with (1, 1), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 11.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 11.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on Dataset 1\n",
      "testing on Dataset 2\n",
      "testing on Dataset 3\n",
      "testing on Dataset 4\n",
      "testing on Dataset 5\n",
      "testing on Dataset 6\n",
      "Training on Dataset 5\n",
      "Iteration with (1, 1), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on Dataset 1\n",
      "testing on Dataset 2\n",
      "testing on Dataset 3\n",
      "testing on Dataset 4\n",
      "testing on Dataset 5\n",
      "testing on Dataset 6\n",
      "Training on Dataset 6\n",
      "Iteration with (1, 1), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  7.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), True, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 29.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), False, max\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 25.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing on Dataset 1\n",
      "testing on Dataset 2\n",
      "testing on Dataset 3\n",
      "testing on Dataset 4\n",
      "testing on Dataset 5\n",
      "testing on Dataset 6\n"
     ]
    }
   ],
   "source": [
    "results_per_model=[]\n",
    "# feature_importance=[]\n",
    "\n",
    "for algo in algorithms:\n",
    "    print(\"------Algo:\", algo)\n",
    "    results=[]\n",
    "    for i,ds_train in enumerate(datasets):\n",
    "\n",
    "        print(\"Training on Dataset {}\".format(i+1))\n",
    "        row=[]\n",
    "\n",
    "        \n",
    "        X_train = ds_train.drop(columns=['Informativeness'])\n",
    "        y_train = ds_train['Informativeness']\n",
    "        X_train_text=X_train['text_processed']\n",
    "        X_train_numerical= X_train.drop(columns=[\"text_processed\"])\n",
    "        \n",
    "        best_score, best_params, best_clf, best_vectorizer, best_tfIdfTransformer = grid_search_text_and_numerical(algo[\"model\"], algo[\"parameters\"], X_train_text, X_train_numerical)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         feature_names = gs_clf.best_estimator_[0].get_feature_names() \n",
    "#         coefs = gs_clf.best_estimator_[-1].coef_[0]\n",
    "#         coefs_with_fns = sorted(zip(coefs, feature_names)) \n",
    "#         df=pd.DataFrame(coefs_with_fns)\n",
    "#         df.columns='coefficient','word'\n",
    "#         df.sort_values(by='coefficient')\n",
    "#         feature_importance.append(df)\n",
    "\n",
    "        for j,ds_test in enumerate(datasets):\n",
    "\n",
    "            print(\"testing on Dataset {}\".format(j+1))\n",
    "            \n",
    "            X_test = ds_test.drop(columns=['Informativeness'])\n",
    "            y_test = ds_test['Informativeness']\n",
    "            X_test_text=X_test['text_processed']\n",
    "            X_test_numerical= X_test.drop(columns=[\"text_processed\"])\n",
    "\n",
    "            row.append(merge_test_and_score(X_test_text, X_test_numerical, best_vectorizer, best_tfIdfTransformer, best_params, best_clf))\n",
    "        results.append(row)\n",
    "    results_per_model.append(results)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.9999984435325532,\n",
       "   0.6532139307410065,\n",
       "   0.672994735650402,\n",
       "   0.6087561819844832,\n",
       "   0.8049242424242424,\n",
       "   0.7016863533988655],\n",
       "  [0.743283233626691,\n",
       "   1.0,\n",
       "   0.741802402920865,\n",
       "   0.5504011850562922,\n",
       "   0.6477272727272727,\n",
       "   0.7795283312632846],\n",
       "  [0.7243200555400572,\n",
       "   0.6865880475988779,\n",
       "   0.9999982310653232,\n",
       "   0.5803350221231633,\n",
       "   0.7803030303030303,\n",
       "   0.7021284393534643],\n",
       "  [0.6525750197439552,\n",
       "   0.5585715260805513,\n",
       "   0.674447384806974,\n",
       "   0.9999993363631673,\n",
       "   0.6458333333333334,\n",
       "   0.6264101429374167],\n",
       "  [0.6969223339964548,\n",
       "   0.5579042731569808,\n",
       "   0.6049618617683686,\n",
       "   0.5606757349321672,\n",
       "   1.0,\n",
       "   0.5620334946047189],\n",
       "  [0.6638264049999828,\n",
       "   0.683753170215264,\n",
       "   0.6559308841843088,\n",
       "   0.6451131110913126,\n",
       "   0.6363636363636365,\n",
       "   0.9999915574999048]]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, svd, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names={0:'random forrest'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i,result in enumerate(results_per_model):\n",
    "    df = pd.DataFrame(result)\n",
    "    df.columns= map( lambda x: dataset_names[x],df.columns)\n",
    "    df.index= map( lambda x: dataset_names[x],df.index)\n",
    "    svm=sns.heatmap(df, annot=True).set_title(model_names[i])\n",
    "    figure = svm.get_figure()    \n",
    "    figure.savefig('./heatmaps/{}.png'.format(model_names[i]), dpi=400)\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results= pd.read_pickle('./reslist.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_auc=list(map( lambda x: list(map(lambda y: y['auc'],x)),results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res_auc)\n",
    "df.columns= map( lambda x: dataset_names[x],df.columns)\n",
    "df.index= map( lambda x: dataset_names[x],df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df, annot=True).set_title('DistilBERT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feature_importance_dataset in enumerate(feature_importance):\n",
    "    feature_importance_dataset.to_csv('./features/{}_features.csv'.format(dataset_names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
