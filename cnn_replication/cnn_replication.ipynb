{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import glob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train features: 57348 \n",
      "Number of test features: 194\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob(\"./dataset_cleaning/*.pkl\")\n",
    "all_files.sort()\n",
    "test_filename= \"./dataset_cleaning/dataset4Cleaned.pkl\"\n",
    "\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    \n",
    "    if(filename != test_filename and filename != test2_filename):\n",
    "\n",
    "        df = pd.read_pickle(filename)\n",
    "        li.append(df)\n",
    "\n",
    "train = pd.concat(li, axis=0, ignore_index=True)\n",
    "test = pd.read_pickle(test_filename)\n",
    "\n",
    "print(\"Number of train features: {} \\nNumber of test features: {}\".format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../dataset_cleaning/dataset1Cleaned.pkl')\n",
    "test = pd.read_pickle('../dataset_cleaning/dataset2Cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Informativeness', 'text', 'positive_score', 'negative_score',\n",
       "       'emotional_devergence_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./../dataset_cleaning/dataset1Cleaned.pkl\n",
      "./../dataset_cleaning/dataset2Cleaned.pkl\n",
      "./../dataset_cleaning/dataset3Cleaned.pkl\n",
      "./../dataset_cleaning/dataset4Cleaned.pkl\n",
      "./../dataset_cleaning/dataset5Cleaned.pkl\n",
      "./../dataset_cleaning/dataset6Cleaned.pkl\n",
      "./../dataset_cleaning/dataset7Cleaned.pkl\n",
      "Number of train features: 45829 \n",
      "Number of test features: 19642\n"
     ]
    }
   ],
   "source": [
    "# all_files = glob.glob(\"./../dataset_cleaning/*.pkl\")\n",
    "# all_files.sort()\n",
    "# datasets = []\n",
    "\n",
    "# for filename in all_files:\n",
    "#     print(filename)\n",
    "#     df = pd.read_pickle(filename)\n",
    "#     datasets.append(df)\n",
    "    \n",
    "# from sklearn.model_selection import train_test_split\n",
    "# df_merged = pd.concat(datasets)\n",
    "# df_merged = df_merged.sample(frac=1).reset_index(drop=True)\n",
    "# train, test= train_test_split(df_merged, test_size=0.3, random_state=42)\n",
    "# print(\"Number of train features: {} \\nNumber of test features: {}\".format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3147: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "authors=pd.read_csv('../dataset_cleaning/tj/tweet_metadata_full.csv')\n",
    "users=pd.read_csv('../dataset_cleaning/tj/twitter_user_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = authors.drop(columns=['Unnamed: 0'])\n",
    "users = users.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.merge(train, authors, on='id')\n",
    "test=pd.merge(test, authors, on='id')\n",
    "train.drop(train.columns.difference(['id', 'Informativeness', 'text', 'author_id', 'tweet_type','positive_score', 'negative_score',\n",
    "       'emotional_devergence_score' ]), 1, inplace=True)\n",
    "test.drop(test.columns.difference(['id', 'Informativeness', 'text', 'author_id', 'tweet_type','positive_score', 'negative_score',\n",
    "       'emotional_devergence_score' ]), 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.drop(columns=['created_at', 'lang', 'name', 'screen_name', 'location','access'], inplace=True)\n",
    "users.columns=['author_id', 'has_description', 'bio_has_url', 'followers_count', 'friends_count',\n",
    "       'favourites_count', 'listed_count', 'statuses_count', 'protected',\n",
    "       'verified', 'default_profile', 'default_profile_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.merge(train, users, on='author_id')\n",
    "test=pd.merge(test, users, on='author_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Informativeness</th>\n",
       "      <th>text</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>emotional_devergence_score</th>\n",
       "      <th>author_id</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>has_description</th>\n",
       "      <th>bio_has_url</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>protected</th>\n",
       "      <th>verified</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>910171633578586113</td>\n",
       "      <td>1</td>\n",
       "      <td>Hurricane Irma shuts down #LGBT businesses in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>99041149</td>\n",
       "      <td>tweet</td>\n",
       "      <td>South Florida Gay News reports on our lives wi...</td>\n",
       "      <td>https://t.co/I92LcgAwrP</td>\n",
       "      <td>5715</td>\n",
       "      <td>5266</td>\n",
       "      <td>1930</td>\n",
       "      <td>269.0</td>\n",
       "      <td>36189</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>505040978776834048</td>\n",
       "      <td>1</td>\n",
       "      <td>#Earthquake M 1.8, Northern California http://...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>542201905</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Independent syndicated #Earthquake #quake #Inf...</td>\n",
       "      <td>http://t.co/gBmygFEo8N</td>\n",
       "      <td>981</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45224</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>246585296437776384</td>\n",
       "      <td>0</td>\n",
       "      <td>#Earthquake M 1.5, Southern California http://...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>542201905</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Independent syndicated #Earthquake #quake #Inf...</td>\n",
       "      <td>http://t.co/gBmygFEo8N</td>\n",
       "      <td>981</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45224</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>384891022033313793</td>\n",
       "      <td>1</td>\n",
       "      <td>#Earthquake M 1.7, Northern California http://...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>542201905</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Independent syndicated #Earthquake #quake #Inf...</td>\n",
       "      <td>http://t.co/gBmygFEo8N</td>\n",
       "      <td>981</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45224</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>384077389871206400</td>\n",
       "      <td>1</td>\n",
       "      <td>#Earthquake M 2.5, Central California http://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>542201905</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Independent syndicated #Earthquake #quake #Inf...</td>\n",
       "      <td>http://t.co/gBmygFEo8N</td>\n",
       "      <td>981</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>45224</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  Informativeness  \\\n",
       "0  910171633578586113                1   \n",
       "1  505040978776834048                1   \n",
       "2  246585296437776384                0   \n",
       "3  384891022033313793                1   \n",
       "4  384077389871206400                1   \n",
       "\n",
       "                                                text  positive_score  \\\n",
       "0  Hurricane Irma shuts down #LGBT businesses in ...               1   \n",
       "1  #Earthquake M 1.8, Northern California http://...               1   \n",
       "2  #Earthquake M 1.5, Southern California http://...               1   \n",
       "3  #Earthquake M 1.7, Northern California http://...               1   \n",
       "4  #Earthquake M 2.5, Central California http://t...               1   \n",
       "\n",
       "   negative_score  emotional_devergence_score  author_id tweet_type  \\\n",
       "0              -1                         0.2   99041149      tweet   \n",
       "1              -1                         0.2  542201905      tweet   \n",
       "2              -1                         0.2  542201905      tweet   \n",
       "3              -1                         0.2  542201905      tweet   \n",
       "4              -1                         0.2  542201905      tweet   \n",
       "\n",
       "                                     has_description              bio_has_url  \\\n",
       "0  South Florida Gay News reports on our lives wi...  https://t.co/I92LcgAwrP   \n",
       "1  Independent syndicated #Earthquake #quake #Inf...   http://t.co/gBmygFEo8N   \n",
       "2  Independent syndicated #Earthquake #quake #Inf...   http://t.co/gBmygFEo8N   \n",
       "3  Independent syndicated #Earthquake #quake #Inf...   http://t.co/gBmygFEo8N   \n",
       "4  Independent syndicated #Earthquake #quake #Inf...   http://t.co/gBmygFEo8N   \n",
       "\n",
       "   followers_count  friends_count  favourites_count  listed_count  \\\n",
       "0             5715           5266              1930         269.0   \n",
       "1              981             59                 0          35.0   \n",
       "2              981             59                 0          35.0   \n",
       "3              981             59                 0          35.0   \n",
       "4              981             59                 0          35.0   \n",
       "\n",
       "   statuses_count  protected  verified  default_profile  default_profile_image  \n",
       "0           36189      False     False            False                  False  \n",
       "1           45224      False     False            False                  False  \n",
       "2           45224      False     False            False                  False  \n",
       "3           45224      False     False            False                  False  \n",
       "4           45224      False     False            False                  False  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_processing(tweet):\n",
    "    \n",
    "    tweet= tweet.lower()\n",
    "    \n",
    "    #Removing hyperlinks from the tweet\n",
    "    tweet_no_links=re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    #Generating the list of words in the tweet (hashtags and other punctuations removed)\n",
    "    def form_sentence(tweet):\n",
    "        tweet_blob = TextBlob(tweet)\n",
    "        return ' '.join(tweet_blob.words)\n",
    "    new_tweet = form_sentence(tweet_no_links)\n",
    "    \n",
    "    #Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(tweet):\n",
    "        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    no_punc_tweet = no_user_alpha(new_tweet)\n",
    "    \n",
    "    #Normalizing the words in tweets \n",
    "    def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return \" \".join(normalized_tweet)\n",
    "    \n",
    "    \n",
    "    return normalization(no_punc_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hashtags(text):\n",
    "    return text.count('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_present(text):\n",
    "    return int('http' in text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def text_has_emoji(text):\n",
    "    return any(map(text[:-50].__contains__, [':)',':(',':P',':c',':<','c:','<:',':L',':l','^_^','^.^','>_<','>.<','>_>','<_<','>.>','<.<','-.-','-_-','o_o','o.o','._.','owo','OwO',';_;','>:)',':]',':}','>:(','>:|','-.^','-_^','8)','B)','<3','xD',':3','x3','\\o','\\o/',';_;','OwO','uwu','O:)',':#',':*']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_instructional_words(text):\n",
    "    return any(map(text.__contains__, [\"text\", 'call', 'donate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_phone_number(text):\n",
    "    return bool(re.search('\\\\d{3}-\\\\d{3}-\\\\d{4}', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_has_slang(text):\n",
    "    for i in text[:-50].split(' '):\n",
    "        for j in ['afaik','afk','asl','atm','atw','ayy','bae','bb','bbiab','bbl','bbs','bc','bf','bff','bork','brb','btw','cba','convo','cp','cya','cya','dank','dc','dem','dw','e2e','fml','FOMO','FTFY','ftl','ftw','fwiw','fyi','g2g','g4u','gf','gg','goml','gr8','gratz','gtfo','guiz','hbu','hru','ianadb','ianalb','ianap','idc','idgaf','idk','iirc','ik','ikr','ily','inb4','irl','jfc','jk','js','k','kappa','kek','kms','kthx','l8r','leet','lmao','lmk','lol','LPT','lrl','lrn2','m8','maga','mfw','mrw','nerf','ngl','nm','nmu','noob','nvm','ofc','omf','omg','omw','ooc','op','OP','orly','pepe','pleb','pleb','plz','pron','pwned','REEEEEE','rekt','rickrol','rip','rly','rms','rofl','rotflol','rtfm','rude','shank','smd','smh','soz','swag','tbf','tbh','tbt','TIFU','tf','tfw','thx','tide','TIL','tl;dr','tmw','tolo','topkek','ty','uwotm8','w00t','wb','wot','wtb','wtf','wtg','wts','wuu2','yarly','ymmv','yolo','yw']:\n",
    "            if(i==j):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_RT(text):\n",
    "    for i in text[:-50].split(' '):\n",
    "        if(i == 'RT'):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_profanity(text):\n",
    "    for i in text[:-50].split(' '):\n",
    "        for j in ['acrotomophilia','anal','anilingus','anus','arsehole','ass','asshole','assmunch','auto erotic','autoerotic','babeland','baby batter','ball gag','ball gravy','ball kicking','ball licking','ball sack','ball sucking','bangbros','bareback','barely legal','barenaked','bastardo','bastinado','bbw','bdsm','beaver cleaver','beaver lips','bestiality','bi curious','big black','big breasts','big knockers','big tits','bimbos','birdlock','bitch','black cock','blonde action','blonde on blonde action','blow j','blow your l','blue waffle','blumpkin','bollocks','bondage','boner','boob','boobs','booty call','brown showers','brunette action','bukkake','bulldyke','bullet vibe','bung hole','bunghole','busty','butt','buttcheeks','butthole','camel toe','camgirl','camslut','camwhore','carpet muncher','carpetmuncher','chocolate rosebuds','circlejerk','cleveland steamer','clit','clitoris','clover clamps','clusterfuck','cock','cocks','coprolagnia','coprophilia','cornhole','cum','cumming','cunnilingus','cunt','darkie','date rape','daterape','deep throat','deepthroat','dick','dildo','dirty pillows','dirty sanchez','dog style','doggie style','doggiestyle','doggy style','doggystyle','dolcett','domination','dominatrix','dommes','donkey punch','double dong','double penetration','dp action','eat my ass','ecchi','ejaculation','erotic','erotism','escort','ethical slut','eunuch','faggot','fecal','felch','fellatio','feltch','female squirting','femdom','figging','fingering','fisting','foot fetish','footjob','frotting','fuck','fucking','fuck buttons','fudge packer','fudgepacker','futanari','g-spot','gang bang','gay sex','genitals','giant cock','girl on','girl on top','girls gone wild','goatcx','goatse','gokkun','golden shower','goo girl','goodpoop','goregasm','grope','group sex','guro','hand job','handjob','hard core','hardcore','hentai','homoerotic','honkey','hooker','hot chick','how to kill','how to murder','huge fat','humping','incest','intercourse','jack off','jail bait','jailbait','jerk off','jigaboo','jiggaboo','jiggerboo','jizz','juggs','kike','kinbaku','kinkster','kinky','knobbing','leather restraint','leather straight jacket','lemon party','lolita','lovemaking','make me come','male squirting','masturbate','menage a trois','milf','missionary position','motherfucker','mound of venus','mr hands','muff diver','muffdiving','nambla','nawashi','negro','neonazi','nig nog','nigga','nigger','nimphomania','nipple','nipples','nsfw images','nude','nudity','nympho','nymphomania','octopussy','omorashi','one cup two girls','one guy one jar','orgasm','orgy','paedophile','panties','panty','pedobear','pedophile','pegging','penis','phone sex','piece of shit','piss pig','pissing','pisspig','playboy','pleasure chest','pole smoker','ponyplay','poof','poop chute','poopchute','porn','porno','pornography','prince albert piercing','pthc','pubes','pussy','queaf','raghead','raging boner','rape','raping','rapist','rectum','reverse cowgirl','rimjob','rimming','rosy palm','rosy palm and her 5 sisters','rusty trombone','s&m','sadism','scat','schlong','scissoring','semen','sex','sexo','sexy','shaved beaver','shaved pussy','shemale','shibari','shit','shota','shrimping','slanteye','slut','smut','snatch','snowballing','sodomize','sodomy','spic','spooge','spread legs','strap on','strapon','strappado','strip club','style doggy','suck','sucks','suicide girls','sultry women','swastika','swinger','tainted love','taste my','tea bagging','threesome','throating','tied up','tight white','tit','tits','titties','titty','tongue in a','topless','tosser','towelhead','tranny','tribadism','tub girl','tubgirl','tushy','twat','twink','twinkie','two girls one cup','undressing','upskirt','urethra play','urophilia','vagina','venus mound','vibrator','violet blue','violet wand','vorarephilia','voyeur','vulva','wank','wet dream','wetback','white power','women rapping','wrapping men','wrinkled starfish','xx','xxx','yaoi','yellow showers','yiffy','zoophilia']:\n",
    "            if(i==j):\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_word_sentance(text):\n",
    "    return int(len(text.split(\" \"))<=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"hashtag_count\"]=train['text'].apply(count_hashtags)\n",
    "test[\"hashtag_count\"]=test['text'].apply(count_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"hashtag_present\"] = train[\"hashtag_count\"].apply(lambda x: np.sign(x))\n",
    "test[\"hashtag_present\"] = test[\"hashtag_count\"].apply(lambda x: np.sign(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"link_present\"]=train['text'].apply(link_present)\n",
    "test[\"link_present\"]=test['text'].apply(link_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"emoji_present\"] = train['text'].apply(text_has_emoji)\n",
    "test[\"emoji_present\"] = test['text'].apply(text_has_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"instructinal_keyword\"] = train['text'].apply(has_instructional_words)\n",
    "test[\"instructinal_keyword\"] = test['text'].apply(has_instructional_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"contains_phone_number\"] = train['text'].apply(has_phone_number)\n",
    "test[\"contains_phone_number\"] = test['text'].apply(has_phone_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"slang_present\"] = train['text'].apply(text_has_slang)\n",
    "test[\"slang_present\"] = test['text'].apply(text_has_slang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"is_RT\"] = train['text'].apply(is_RT)\n",
    "test[\"is_RT\"] = test['text'].apply(is_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"has_profanity\"] = train['text'].apply(has_profanity)\n",
    "test[\"has_profanity\"] = test['text'].apply(has_profanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"one_word_sentance\"] = train['text'].apply(one_word_sentance)\n",
    "test[\"one_word_sentance\"] = test['text'].apply(one_word_sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.apply(get_sentiment_score, axis=1)\n",
    "# test = test.apply(get_sentiment_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"emotional_devergence_score\"]=(train[\"positive_score\"]-train[\"negative_score\"])/10.0\n",
    "# test[\"emotional_devergence_score\"]=(test[\"positive_score\"]-test[\"negative_score\"])/10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Informativeness</th>\n",
       "      <th>text</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>emotional_devergence_score</th>\n",
       "      <th>author_id</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>has_description</th>\n",
       "      <th>bio_has_url</th>\n",
       "      <th>...</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>hashtag_present</th>\n",
       "      <th>link_present</th>\n",
       "      <th>emoji_present</th>\n",
       "      <th>instructinal_keyword</th>\n",
       "      <th>contains_phone_number</th>\n",
       "      <th>slang_present</th>\n",
       "      <th>is_RT</th>\n",
       "      <th>has_profanity</th>\n",
       "      <th>one_word_sentance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>207704417108103169</td>\n",
       "      <td>1</td>\n",
       "      <td>DTN World News: Old woman polled out alive in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>137840213</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Comprehensive Daily News on The World of Today...</td>\n",
       "      <td>http://defense-technologynews.blogspot.com/</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>959839199389036546</td>\n",
       "      <td>0</td>\n",
       "      <td>This is horrifying that all over the world the...</td>\n",
       "      <td>2</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>462478512</td>\n",
       "      <td>quote</td>\n",
       "      <td>#IFB 💯public school teacher for 20+ yrs. Proud...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>850353395818954752</td>\n",
       "      <td>0</td>\n",
       "      <td>#prayforstockholm Please, just please, dont us...</td>\n",
       "      <td>3</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1149393234</td>\n",
       "      <td>tweet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>383510892027330560</td>\n",
       "      <td>0</td>\n",
       "      <td>Honestly I lost the count of fake degree holde...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>572666480</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Die hard worker of #MQM. Love #AltafHussain</td>\n",
       "      <td>http://t.co/hDJCrkJilk</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>910135800750268416</td>\n",
       "      <td>1</td>\n",
       "      <td>https://t.co/IT1aCXmNFQ : HHS Offers Special M...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>164865134</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Putting the Social in Supply Chain. The gather...</td>\n",
       "      <td>https://t.co/EjRpsCiQJi</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  Informativeness  \\\n",
       "0  207704417108103169                1   \n",
       "1  959839199389036546                0   \n",
       "2  850353395818954752                0   \n",
       "3  383510892027330560                0   \n",
       "4  910135800750268416                1   \n",
       "\n",
       "                                                text  positive_score  \\\n",
       "0  DTN World News: Old woman polled out alive in ...               1   \n",
       "1  This is horrifying that all over the world the...               2   \n",
       "2  #prayforstockholm Please, just please, dont us...               3   \n",
       "3  Honestly I lost the count of fake degree holde...               1   \n",
       "4  https://t.co/IT1aCXmNFQ : HHS Offers Special M...               1   \n",
       "\n",
       "   negative_score  emotional_devergence_score   author_id tweet_type  \\\n",
       "0              -1                         0.2   137840213      tweet   \n",
       "1              -4                         0.6   462478512      quote   \n",
       "2              -4                         0.7  1149393234      tweet   \n",
       "3              -2                         0.3   572666480      tweet   \n",
       "4              -1                         0.2   164865134      tweet   \n",
       "\n",
       "                                     has_description  \\\n",
       "0  Comprehensive Daily News on The World of Today...   \n",
       "1  #IFB 💯public school teacher for 20+ yrs. Proud...   \n",
       "2                                                NaN   \n",
       "3        Die hard worker of #MQM. Love #AltafHussain   \n",
       "4  Putting the Social in Supply Chain. The gather...   \n",
       "\n",
       "                                   bio_has_url  ...  hashtag_count  \\\n",
       "0  http://defense-technologynews.blogspot.com/  ...              0   \n",
       "1                                          NaN  ...              0   \n",
       "2                                          NaN  ...              1   \n",
       "3                       http://t.co/hDJCrkJilk  ...              2   \n",
       "4                      https://t.co/EjRpsCiQJi  ...              0   \n",
       "\n",
       "   hashtag_present  link_present  emoji_present  instructinal_keyword  \\\n",
       "0                0             1          False                 False   \n",
       "1                0             1          False                 False   \n",
       "2                1             0          False                 False   \n",
       "3                1             0          False                 False   \n",
       "4                0             1          False                 False   \n",
       "\n",
       "   contains_phone_number  slang_present  is_RT  has_profanity  \\\n",
       "0                  False          False  False          False   \n",
       "1                  False          False  False          False   \n",
       "2                  False          False  False          False   \n",
       "3                  False          False  False          False   \n",
       "4                  False          False  False          False   \n",
       "\n",
       "   one_word_sentance  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'author_id',\n",
       " 'tweet_type',\n",
       " 'has_description',\n",
       " 'bio_has_url',\n",
       " 'protected',\n",
       " 'default_profile',\n",
       " 'default_profile_image']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['id',  'author_id', 'tweet_type','has_description', 'bio_has_url','protected', 'default_profile','default_profile_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(columns=['id',  'author_id', 'tweet_type','has_description', 'bio_has_url','protected', 'default_profile','default_profile_image'])\n",
    "test= test.drop(columns=['id',  'author_id', 'tweet_type','has_description', 'bio_has_url','protected', 'default_profile','default_profile_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:701: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 37203/37203 [02:14<00:00, 276.03it/s]\n",
      "100%|██████████| 15971/15971 [01:30<00:00, 176.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "train['text']=train['text'].progress_apply(text_processing)\n",
    "test['text']=test['text'].progress_apply(text_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = np.asarray(train.Informativeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37909 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "texts=train.text\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train : (37203, 464)\n",
      "Shape of label train : (37203,)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(sequences_train)\n",
    "y_train = np.asarray(train.Informativeness)\n",
    "print('Shape of X train :', X_train.shape)\n",
    "print('Shape of label train :', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train.text.apply(lambda x: len(x.split(' '))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../word2vec_data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=len(word_index)+1\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37910, 300)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37910, 300)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37203, 21)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "sequence_length = X_train.shape[1]\n",
    "\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 150\n",
    "drop = 0.5\n",
    "\n",
    "\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "\n",
    "#CHANGE THIS!\n",
    "meta_input = Input(shape=(19,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "merged_tensor2 = concatenate([dropout, meta_input])\n",
    "\n",
    "output = Dense(units=2, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(merged_tensor2)\n",
    "\n",
    "\n",
    "\n",
    "model = Model([inputs , meta_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "y_train_extra = np.abs(y_train-1)\n",
    "\n",
    "X_train_meta = train.to_numpy()[:, 2:].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 896s 24s/step - loss: 5495.5020 - acc: 0.6644\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 872s 23s/step - loss: 2296.1868 - acc: 0.6178\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 800s 21s/step - loss: 295.1976 - acc: 0.6324\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 897s 24s/step - loss: 61.7619 - acc: 0.7471\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 845s 22s/step - loss: 39.3861 - acc: 0.7699\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 777s 20s/step - loss: 74.8804 - acc: 0.7641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b2a657390>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "callbacks = [EarlyStopping(monitor='loss')]\n",
    "model.fit([X_train, X_train_meta], np.append(y_train, y_train_extra, axis=1), batch_size=1000, epochs=10, verbose=1,\n",
    "         callbacks=callbacks)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Informativeness', 'text', 'positive_score', 'negative_score',\n",
       "       'emotional_devergence_score', 'followers_count', 'friends_count',\n",
       "       'favourites_count', 'listed_count', 'statuses_count', 'verified',\n",
       "       'hashtag_count', 'hashtag_present', 'link_present', 'emoji_present',\n",
       "       'instructinal_keyword', 'contains_phone_number', 'slang_present',\n",
       "       'is_RT', 'has_profanity', 'one_word_sentance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Informativeness', 'text', 'positive_score', 'negative_score',\n",
       "       'emotional_devergence_score', 'followers_count', 'friends_count',\n",
       "       'favourites_count', 'listed_count', 'statuses_count', 'verified',\n",
       "       'hashtag_count', 'hashtag_present', 'link_present', 'emoji_present',\n",
       "       'instructinal_keyword', 'contains_phone_number', 'slang_present',\n",
       "       'is_RT', 'has_profanity', 'one_word_sentance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test=tokenizer.texts_to_sequences(test.text)\n",
    "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n",
    "X_test_meta = test.to_numpy()[:, 2:].astype('float32')\n",
    "y_pred=model.predict([X_test, X_test_meta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bool =(y_pred > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.asarray(test.Informativeness).reshape(len(test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.7156721557823555, f1: 0.7841011743450768, roc: 0.6945941640545097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"Acc: {}, f1: {}, roc: {}\".format(accuracy_score(y_true, y_pred_bool[:,0]),f1_score(y_true, y_pred_bool[:,0]),roc_auc_score(y_true, y_pred_bool[:,0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
