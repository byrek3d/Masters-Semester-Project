{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import glob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train features: 57348 \n",
      "Number of test features: 194\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob(\"./dataset_cleaning/*.pkl\")\n",
    "all_files.sort()\n",
    "test_filename= \"./dataset_cleaning/dataset4Cleaned.pkl\"\n",
    "test2_filename= \"./dataset_cleaning/dataset7Cleaned.pkl\"\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    \n",
    "    if(filename != test_filename and filename != test2_filename):\n",
    "\n",
    "        df = pd.read_pickle(filename)\n",
    "        li.append(df)\n",
    "\n",
    "train = pd.concat(li, axis=0, ignore_index=True)\n",
    "test = pd.read_pickle(test2_filename)\n",
    "\n",
    "print(\"Number of train features: {} \\nNumber of test features: {}\".format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../dataset_cleaning/dataset1Cleaned.pkl')\n",
    "test = pd.read_pickle('../dataset_cleaning/dataset2Cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Informativeness', 'text', 'positive_score', 'negative_score',\n",
       "       'emotional_devergence_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "authors=pd.read_csv('../dataset_cleaning/tj/tweet_metadata_full.csv')\n",
    "users=pd.read_csv('../dataset_cleaning/tj/twitter_user_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = authors.drop(columns=['Unnamed: 0'])\n",
    "users = users.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.merge(train, authors, on='id')\n",
    "test=pd.merge(test, authors, on='id')\n",
    "train.drop(train.columns.difference(['id', 'Informativeness', 'text', 'author_id', 'tweet_type','positive_score', 'negative_score',\n",
    "       'emotional_devergence_score' ]), 1, inplace=True)\n",
    "test.drop(test.columns.difference(['id', 'Informativeness', 'text', 'author_id', 'tweet_type','positive_score', 'negative_score',\n",
    "       'emotional_devergence_score' ]), 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.drop(columns=['created_at', 'lang', 'name', 'screen_name', 'location','access'], inplace=True)\n",
    "users.columns=['author_id', 'has_description', 'bio_has_url', 'followers_count', 'friends_count',\n",
    "       'favourites_count', 'listed_count', 'statuses_count', 'protected',\n",
    "       'verified', 'default_profile', 'default_profile_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.merge(train, users, on='author_id')\n",
    "test=pd.merge(test, users, on='author_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Informativeness</th>\n",
       "      <th>text</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>emotional_devergence_score</th>\n",
       "      <th>author_id</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>has_description</th>\n",
       "      <th>bio_has_url</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>protected</th>\n",
       "      <th>verified</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211040709124440064</td>\n",
       "      <td>0</td>\n",
       "      <td>#Intern #US #TATTOO #Wisconsin #Ohio #NC #PA #...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>601864285</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Traveling http://goo.gl/97yT2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>402</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210864180218167296</td>\n",
       "      <td>0</td>\n",
       "      <td>Get in on the fun every Thursday with the @csi...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>245545247</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Director of Marketing &amp; Promotions for the Col...</td>\n",
       "      <td>http://www.csindy.com/colorado/Home</td>\n",
       "      <td>454</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3327</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>211157222699433985</td>\n",
       "      <td>0</td>\n",
       "      <td>Welcome to our newest STUDENTathlete- Reagan B...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>487854217</td>\n",
       "      <td>tweet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>422</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>211162553659830272</td>\n",
       "      <td>0</td>\n",
       "      <td>Denver Post: #Colorado governor signs bill cre...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>17602802</td>\n",
       "      <td>tweet</td>\n",
       "      <td>The Reporters Committee for Freedom of the Pre...</td>\n",
       "      <td>http://www.rcfp.org</td>\n",
       "      <td>1965</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>4392</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>211216962162933761</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretty sure I'm going to live in Manitou Sprin...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>348495572</td>\n",
       "      <td>tweet</td>\n",
       "      <td>free-spirited, ever thinking, world citizen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>227</td>\n",
       "      <td>203</td>\n",
       "      <td>23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3687</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  Informativeness  \\\n",
       "0  211040709124440064                0   \n",
       "1  210864180218167296                0   \n",
       "2  211157222699433985                0   \n",
       "3  211162553659830272                0   \n",
       "4  211216962162933761                0   \n",
       "\n",
       "                                                text  positive_score  \\\n",
       "0  #Intern #US #TATTOO #Wisconsin #Ohio #NC #PA #...               1   \n",
       "1  Get in on the fun every Thursday with the @csi...               2   \n",
       "2  Welcome to our newest STUDENTathlete- Reagan B...               2   \n",
       "3  Denver Post: #Colorado governor signs bill cre...               1   \n",
       "4  Pretty sure I'm going to live in Manitou Sprin...               3   \n",
       "\n",
       "   negative_score  emotional_devergence_score  author_id tweet_type  \\\n",
       "0              -1                         0.2  601864285      tweet   \n",
       "1              -1                         0.3  245545247      tweet   \n",
       "2              -1                         0.3  487854217      tweet   \n",
       "3              -1                         0.2   17602802      tweet   \n",
       "4              -1                         0.4  348495572      tweet   \n",
       "\n",
       "                                     has_description  \\\n",
       "0                      Traveling http://goo.gl/97yT2   \n",
       "1  Director of Marketing & Promotions for the Col...   \n",
       "2                                                NaN   \n",
       "3  The Reporters Committee for Freedom of the Pre...   \n",
       "4        free-spirited, ever thinking, world citizen   \n",
       "\n",
       "                           bio_has_url  followers_count  friends_count  \\\n",
       "0                                  NaN               62             19   \n",
       "1  http://www.csindy.com/colorado/Home              454             35   \n",
       "2                                  NaN                1              1   \n",
       "3                  http://www.rcfp.org             1965            188   \n",
       "4                                  NaN              227            203   \n",
       "\n",
       "   favourites_count  listed_count  statuses_count  protected  verified  \\\n",
       "0                 0           0.0             402      False     False   \n",
       "1                 2          28.0            3327      False     False   \n",
       "2                 0           0.0             422      False     False   \n",
       "3                 0         181.0            4392      False     False   \n",
       "4                23           2.0            3687      False     False   \n",
       "\n",
       "   default_profile  default_profile_image  \n",
       "0             True                   True  \n",
       "1            False                  False  \n",
       "2             True                   True  \n",
       "3            False                  False  \n",
       "4             True                  False  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_processing(tweet):\n",
    "    \n",
    "    tweet= tweet.lower()\n",
    "    \n",
    "    #Removing hyperlinks from the tweet\n",
    "    tweet_no_links=re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    #Generating the list of words in the tweet (hashtags and other punctuations removed)\n",
    "    def form_sentence(tweet):\n",
    "        tweet_blob = TextBlob(tweet)\n",
    "        return ' '.join(tweet_blob.words)\n",
    "    new_tweet = form_sentence(tweet_no_links)\n",
    "    \n",
    "    #Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(tweet):\n",
    "        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    no_punc_tweet = no_user_alpha(new_tweet)\n",
    "    \n",
    "    #Normalizing the words in tweets \n",
    "    def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return \" \".join(normalized_tweet)\n",
    "    \n",
    "    \n",
    "    return normalization(no_punc_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hashtags(text):\n",
    "    return text.count('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_present(text):\n",
    "    return int('http' in text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def text_has_emoji(text):\n",
    "    return any(map(text[:-50].__contains__, [':)',':(',':P',':c',':<','c:','<:',':L',':l','^_^','^.^','>_<','>.<','>_>','<_<','>.>','<.<','-.-','-_-','o_o','o.o','._.','owo','OwO',';_;','>:)',':]',':}','>:(','>:|','-.^','-_^','8)','B)','<3','xD',':3','x3','\\o','\\o/',';_;','OwO','uwu','O:)',':#',':*']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_instructional_words(text):\n",
    "    return any(map(text.__contains__, [\"text\", 'call', 'donate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_phone_number(text):\n",
    "    return bool(re.search('\\\\d{3}-\\\\d{3}-\\\\d{4}', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_has_slang(text):\n",
    "    for i in text[:-50].split(' '):\n",
    "        for j in ['afaik','afk','asl','atm','atw','ayy','bae','bb','bbiab','bbl','bbs','bc','bf','bff','bork','brb','btw','cba','convo','cp','cya','cya','dank','dc','dem','dw','e2e','fml','FOMO','FTFY','ftl','ftw','fwiw','fyi','g2g','g4u','gf','gg','goml','gr8','gratz','gtfo','guiz','hbu','hru','ianadb','ianalb','ianap','idc','idgaf','idk','iirc','ik','ikr','ily','inb4','irl','jfc','jk','js','k','kappa','kek','kms','kthx','l8r','leet','lmao','lmk','lol','LPT','lrl','lrn2','m8','maga','mfw','mrw','nerf','ngl','nm','nmu','noob','nvm','ofc','omf','omg','omw','ooc','op','OP','orly','pepe','pleb','pleb','plz','pron','pwned','REEEEEE','rekt','rickrol','rip','rly','rms','rofl','rotflol','rtfm','rude','shank','smd','smh','soz','swag','tbf','tbh','tbt','TIFU','tf','tfw','thx','tide','TIL','tl;dr','tmw','tolo','topkek','ty','uwotm8','w00t','wb','wot','wtb','wtf','wtg','wts','wuu2','yarly','ymmv','yolo','yw']:\n",
    "            if(i==j):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_RT(text):\n",
    "    for i in text[:-50].split(' '):\n",
    "        if(i == 'RT'):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_profanity(text):\n",
    "    for i in text[:-50].split(' '):\n",
    "        for j in ['acrotomophilia','anal','anilingus','anus','arsehole','ass','asshole','assmunch','auto erotic','autoerotic','babeland','baby batter','ball gag','ball gravy','ball kicking','ball licking','ball sack','ball sucking','bangbros','bareback','barely legal','barenaked','bastardo','bastinado','bbw','bdsm','beaver cleaver','beaver lips','bestiality','bi curious','big black','big breasts','big knockers','big tits','bimbos','birdlock','bitch','black cock','blonde action','blonde on blonde action','blow j','blow your l','blue waffle','blumpkin','bollocks','bondage','boner','boob','boobs','booty call','brown showers','brunette action','bukkake','bulldyke','bullet vibe','bung hole','bunghole','busty','butt','buttcheeks','butthole','camel toe','camgirl','camslut','camwhore','carpet muncher','carpetmuncher','chocolate rosebuds','circlejerk','cleveland steamer','clit','clitoris','clover clamps','clusterfuck','cock','cocks','coprolagnia','coprophilia','cornhole','cum','cumming','cunnilingus','cunt','darkie','date rape','daterape','deep throat','deepthroat','dick','dildo','dirty pillows','dirty sanchez','dog style','doggie style','doggiestyle','doggy style','doggystyle','dolcett','domination','dominatrix','dommes','donkey punch','double dong','double penetration','dp action','eat my ass','ecchi','ejaculation','erotic','erotism','escort','ethical slut','eunuch','faggot','fecal','felch','fellatio','feltch','female squirting','femdom','figging','fingering','fisting','foot fetish','footjob','frotting','fuck','fucking','fuck buttons','fudge packer','fudgepacker','futanari','g-spot','gang bang','gay sex','genitals','giant cock','girl on','girl on top','girls gone wild','goatcx','goatse','gokkun','golden shower','goo girl','goodpoop','goregasm','grope','group sex','guro','hand job','handjob','hard core','hardcore','hentai','homoerotic','honkey','hooker','hot chick','how to kill','how to murder','huge fat','humping','incest','intercourse','jack off','jail bait','jailbait','jerk off','jigaboo','jiggaboo','jiggerboo','jizz','juggs','kike','kinbaku','kinkster','kinky','knobbing','leather restraint','leather straight jacket','lemon party','lolita','lovemaking','make me come','male squirting','masturbate','menage a trois','milf','missionary position','motherfucker','mound of venus','mr hands','muff diver','muffdiving','nambla','nawashi','negro','neonazi','nig nog','nigga','nigger','nimphomania','nipple','nipples','nsfw images','nude','nudity','nympho','nymphomania','octopussy','omorashi','one cup two girls','one guy one jar','orgasm','orgy','paedophile','panties','panty','pedobear','pedophile','pegging','penis','phone sex','piece of shit','piss pig','pissing','pisspig','playboy','pleasure chest','pole smoker','ponyplay','poof','poop chute','poopchute','porn','porno','pornography','prince albert piercing','pthc','pubes','pussy','queaf','raghead','raging boner','rape','raping','rapist','rectum','reverse cowgirl','rimjob','rimming','rosy palm','rosy palm and her 5 sisters','rusty trombone','s&m','sadism','scat','schlong','scissoring','semen','sex','sexo','sexy','shaved beaver','shaved pussy','shemale','shibari','shit','shota','shrimping','slanteye','slut','smut','snatch','snowballing','sodomize','sodomy','spic','spooge','spread legs','strap on','strapon','strappado','strip club','style doggy','suck','sucks','suicide girls','sultry women','swastika','swinger','tainted love','taste my','tea bagging','threesome','throating','tied up','tight white','tit','tits','titties','titty','tongue in a','topless','tosser','towelhead','tranny','tribadism','tub girl','tubgirl','tushy','twat','twink','twinkie','two girls one cup','undressing','upskirt','urethra play','urophilia','vagina','venus mound','vibrator','violet blue','violet wand','vorarephilia','voyeur','vulva','wank','wet dream','wetback','white power','women rapping','wrapping men','wrinkled starfish','xx','xxx','yaoi','yellow showers','yiffy','zoophilia']:\n",
    "            if(i==j):\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_word_sentance(text):\n",
    "    return int(len(text.split(\" \"))<=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"hashtag_count\"]=train['text'].apply(count_hashtags)\n",
    "test[\"hashtag_count\"]=test['text'].apply(count_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"hashtag_present\"] = train[\"hashtag_count\"].apply(lambda x: np.sign(x))\n",
    "test[\"hashtag_present\"] = test[\"hashtag_count\"].apply(lambda x: np.sign(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"link_present\"]=train['text'].apply(link_present)\n",
    "test[\"link_present\"]=test['text'].apply(link_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"emoji_present\"] = train['text'].apply(text_has_emoji)\n",
    "test[\"emoji_present\"] = test['text'].apply(text_has_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"instructinal_keyword\"] = train['text'].apply(has_instructional_words)\n",
    "test[\"instructinal_keyword\"] = test['text'].apply(has_instructional_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"contains_phone_number\"] = train['text'].apply(has_phone_number)\n",
    "test[\"contains_phone_number\"] = test['text'].apply(has_phone_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"slang_present\"] = train['text'].apply(text_has_slang)\n",
    "test[\"slang_present\"] = test['text'].apply(text_has_slang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"is_RT\"] = train['text'].apply(is_RT)\n",
    "test[\"is_RT\"] = test['text'].apply(is_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"has_profanity\"] = train['text'].apply(has_profanity)\n",
    "test[\"has_profanity\"] = test['text'].apply(has_profanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"one_word_sentance\"] = train['text'].apply(one_word_sentance)\n",
    "test[\"one_word_sentance\"] = test['text'].apply(one_word_sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.apply(get_sentiment_score, axis=1)\n",
    "# test = test.apply(get_sentiment_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"emotional_devergence_score\"]=(train[\"positive_score\"]-train[\"negative_score\"])/10.0\n",
    "# test[\"emotional_devergence_score\"]=(test[\"positive_score\"]-test[\"negative_score\"])/10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Informativeness</th>\n",
       "      <th>text</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>emotional_devergence_score</th>\n",
       "      <th>author_id</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>has_description</th>\n",
       "      <th>bio_has_url</th>\n",
       "      <th>...</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>hashtag_present</th>\n",
       "      <th>link_present</th>\n",
       "      <th>emoji_present</th>\n",
       "      <th>instructinal_keyword</th>\n",
       "      <th>contains_phone_number</th>\n",
       "      <th>slang_present</th>\n",
       "      <th>is_RT</th>\n",
       "      <th>has_profanity</th>\n",
       "      <th>one_word_sentance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917791130590183424</td>\n",
       "      <td>1</td>\n",
       "      <td>PHOTOS: Deadly wildfires rage in California ht...</td>\n",
       "      <td>1</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>23085284</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Real-time updates from WMTW News 8: Maine's To...</td>\n",
       "      <td>https://t.co/hJRuLYj5zC</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>914624000625541121</td>\n",
       "      <td>1</td>\n",
       "      <td>Family helps community by cooking meals for do...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>23085284</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Real-time updates from WMTW News 8: Maine's To...</td>\n",
       "      <td>https://t.co/hJRuLYj5zC</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>765553757887266816</td>\n",
       "      <td>retweet</td>\n",
       "      <td>Official Twitter account of Alerts Made Easy®....</td>\n",
       "      <td>https://t.co/NTpDIXXuLv</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>917792092100988929</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @TIME: California's raging wildfires as you...</td>\n",
       "      <td>1</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1143801686</td>\n",
       "      <td>retweet</td>\n",
       "      <td>Anarchist furry trans pan degenerate that play...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917792147700465664</td>\n",
       "      <td>1</td>\n",
       "      <td>Wildfires Threaten Californiaâ€™s First Legal ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>743291168092364805</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Getting out the latest information in the Cann...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  Informativeness  \\\n",
       "0  917791130590183424                1   \n",
       "1  914624000625541121                1   \n",
       "2  917791291823591425                1   \n",
       "3  917792092100988929                1   \n",
       "4  917792147700465664                1   \n",
       "\n",
       "                                                text  positive_score  \\\n",
       "0  PHOTOS: Deadly wildfires rage in California ht...               1   \n",
       "1  Family helps community by cooking meals for do...               1   \n",
       "2  RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...               1   \n",
       "3  RT @TIME: California's raging wildfires as you...               1   \n",
       "4  Wildfires Threaten Californiaâ€™s First Legal ...               1   \n",
       "\n",
       "   negative_score  emotional_devergence_score           author_id tweet_type  \\\n",
       "0              -4                         0.5            23085284      tweet   \n",
       "1              -1                         0.2            23085284      tweet   \n",
       "2              -1                         0.2  765553757887266816    retweet   \n",
       "3              -4                         0.5          1143801686    retweet   \n",
       "4              -1                         0.2  743291168092364805      tweet   \n",
       "\n",
       "                                     has_description              bio_has_url  \\\n",
       "0  Real-time updates from WMTW News 8: Maine's To...  https://t.co/hJRuLYj5zC   \n",
       "1  Real-time updates from WMTW News 8: Maine's To...  https://t.co/hJRuLYj5zC   \n",
       "2  Official Twitter account of Alerts Made Easy®....  https://t.co/NTpDIXXuLv   \n",
       "3  Anarchist furry trans pan degenerate that play...                      NaN   \n",
       "4  Getting out the latest information in the Cann...                      NaN   \n",
       "\n",
       "   ...  hashtag_count  hashtag_present  link_present  emoji_present  \\\n",
       "0  ...              0                0             1          False   \n",
       "1  ...              0                0             1          False   \n",
       "2  ...              0                0             1          False   \n",
       "3  ...              0                0             1          False   \n",
       "4  ...              0                0             1          False   \n",
       "\n",
       "   instructinal_keyword  contains_phone_number  slang_present  is_RT  \\\n",
       "0                 False                  False          False  False   \n",
       "1                 False                  False          False  False   \n",
       "2                 False                  False          False   True   \n",
       "3                 False                  False          False   True   \n",
       "4                 False                  False          False  False   \n",
       "\n",
       "   has_profanity  one_word_sentance  \n",
       "0          False                  0  \n",
       "1          False                  0  \n",
       "2          False                  0  \n",
       "3          False                  0  \n",
       "4          False                  0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'author_id',\n",
       " 'tweet_type',\n",
       " 'has_description',\n",
       " 'bio_has_url',\n",
       " 'protected',\n",
       " 'default_profile',\n",
       " 'default_profile_image']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['id',  'author_id', 'tweet_type','has_description', 'bio_has_url','protected', 'default_profile','default_profile_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(columns=['id',  'author_id', 'tweet_type','has_description', 'bio_has_url','protected', 'default_profile','default_profile_image'])\n",
    "test= test.drop(columns=['id',  'author_id', 'tweet_type','has_description', 'bio_has_url','protected', 'default_profile','default_profile_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "100%|██████████| 245M/245M [02:18<00:00, 1.77MB/s] \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "st_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 18154/18154 [01:10<00:00, 256.67it/s]\n",
      "100%|██████████| 12417/12417 [00:34<00:00, 359.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "train['text']=train['text'].progress_apply(text_processing)\n",
    "test['text']=test['text'].progress_apply(text_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_data_train = st_model.encode(train[\"text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = np.asarray(train.Informativeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39774 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "texts=train.text\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train : (47729, 26)\n",
      "Shape of label train : (47729,)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(sequences_train)\n",
    "y_train = np.asarray(train.Informativeness)\n",
    "print('Shape of X train :', X_train.shape)\n",
    "print('Shape of label train :', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train.text.apply(lambda x: len(x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./word2vec_data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=len(word_index)+1\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39775, 300)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47729, 21)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged, input_shape = [768], output_shape = [2, 768]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-50cd6e277605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmeta_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# embedding = embedding_layer(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mreshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# reshape=inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 926\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1115\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m               \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    542\u001b[0m       \u001b[0;31m# Set the static shape for the result since it might lost during array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;31m# reshape, eg, some `None` dim in the result could be inferred.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m       \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcompute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    533\u001b[0m       \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m       output_shape += self._fix_unknown_dimension(input_shape[1:],\n\u001b[0;32m--> 535\u001b[0;31m                                                   self.target_shape)\n\u001b[0m\u001b[1;32m    536\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m_fix_unknown_dimension\u001b[0;34m(self, input_shape, output_shape)\u001b[0m\n\u001b[1;32m    521\u001b[0m       \u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munknown\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged, input_shape = [768], output_shape = [2, 768]"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "EMBEDDING_DIM=300\n",
    "sequence_length = bert_data_train.shape[1]\n",
    "\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 150\n",
    "drop = 0.5\n",
    "\n",
    "\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "\n",
    "#CHANGE THIS!\n",
    "meta_input = Input(shape=(19,))\n",
    "# embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((2,sequence_length))(inputs)\n",
    "\n",
    "# reshape=inputs\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], sequence_length),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], sequence_length),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], sequence_length),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "merged_tensor2 = concatenate([dropout, meta_input])\n",
    "\n",
    "output = Dense(units=2, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(merged_tensor2)\n",
    "\n",
    "\n",
    "\n",
    "model = Model([inputs , meta_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "y_train_extra = np.abs(y_train-1)\n",
    "\n",
    "X_train_meta = train.to_numpy()[:, 2:].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18154, 19)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18154, 768)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "478/478 [==============================] - 151s 317ms/step - loss: 424.7287 - auc: 0.7222\n",
      "Epoch 2/10\n",
      "478/478 [==============================] - 144s 301ms/step - loss: 158.2584 - auc: 0.7982\n",
      "Epoch 3/10\n",
      "478/478 [==============================] - 139s 292ms/step - loss: 201.7319 - auc: 0.8341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3b61dad0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['AUC'])\n",
    "callbacks = [EarlyStopping(monitor='loss')]\n",
    "model.fit([bert_data_train, X_train_meta], np.append(y_train, y_train_extra, axis=1), batch_size=100, epochs=10, verbose=1,\n",
    "         callbacks=callbacks)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Informativeness', 'text', 'positive_score', 'negative_score',\n",
       "       'emotional_devergence_score', 'followers_count', 'friends_count',\n",
       "       'favourites_count', 'listed_count', 'statuses_count', 'verified',\n",
       "       'hashtag_count', 'hashtag_present', 'link_present', 'emoji_present',\n",
       "       'instructinal_keyword', 'contains_phone_number', 'slang_present',\n",
       "       'is_RT', 'has_profanity', 'one_word_sentance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Informativeness', 'text', 'positive_score', 'negative_score',\n",
       "       'emotional_devergence_score', 'followers_count', 'friends_count',\n",
       "       'favourites_count', 'listed_count', 'statuses_count', 'verified',\n",
       "       'hashtag_count', 'hashtag_present', 'link_present', 'emoji_present',\n",
       "       'instructinal_keyword', 'contains_phone_number', 'slang_present',\n",
       "       'is_RT', 'has_profanity', 'one_word_sentance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test=tokenizer.texts_to_sequences(test.text)\n",
    "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n",
    "X_test_meta = test.to_numpy()[:, 2:].astype('float32')\n",
    "y_pred=model.predict([X_test, X_test_meta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bool =(y_pred > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.asarray(test.Informativeness).reshape(len(test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.6517412935323383, f1: 0.4166666666666667, roc: 0.5842198581560284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"Acc: {}, f1: {}, roc: {}\".format(accuracy_score(y_true, y_pred_bool[:,0]),f1_score(y_true, y_pred_bool[:,0]),roc_auc_score(y_true, y_pred_bool[:,0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
