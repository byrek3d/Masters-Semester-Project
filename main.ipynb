{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1600856181231,
     "user": {
      "displayName": "byrek",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAvHM_OBXbApiIG8MkYPz0OhB3fW-xzyqHesPi-w=s64",
      "userId": "17956107666140341860"
     },
     "user_tz": -120
    },
    "id": "IdCF_BuqdrFc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import glob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PuwW_15sLSBu"
   },
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train features: 47317 \n",
      "Number of test features: 18154\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob(\"./dataset_cleaning/*.pkl\")\n",
    "all_files.sort()\n",
    "test_filename= \"./dataset_cleaning/dataset1Cleaned.pkl\"\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    \n",
    "    if(filename != test_filename):\n",
    "\n",
    "        df = pd.read_pickle(filename)\n",
    "        li.append(df)\n",
    "\n",
    "train_df = pd.concat(li, axis=0, ignore_index=True)\n",
    "test_df = pd.read_pickle(test_filename)\n",
    "\n",
    "print(\"Number of train features: {} \\nNumber of test features: {}\".format(len(train_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train features: 45829 \n",
      "Number of test features: 19642\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# df_merged = pd.concat([train_df, test_df])\n",
    "# df_merged = df_merged.sample(frac=1).reset_index(drop=True)\n",
    "# train_df, test_df= train_test_split(df_merged, test_size=0.3, random_state=42)\n",
    "# print(\"Number of train features: {} \\nNumber of test features: {}\".format(len(train_df), len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Informativeness</th>\n",
       "      <th>text</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>emotional_devergence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43067</th>\n",
       "      <td>243372363088003072</td>\n",
       "      <td>1</td>\n",
       "      <td>BREAKING NEWS...\\n\\nA powerful 7.6-magnitude e...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11196</th>\n",
       "      <td>409458777738731520</td>\n",
       "      <td>1</td>\n",
       "      <td>Funeral held for Kisook Ahn, one of four passe...</td>\n",
       "      <td>1</td>\n",
       "      <td>-4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>390774640202629120</td>\n",
       "      <td>0</td>\n",
       "      <td>Thank you to the thousands of firefighters who...</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55849</th>\n",
       "      <td>348128787155795971</td>\n",
       "      <td>0</td>\n",
       "      <td>PHOTO: The red circle is the train tunnel into...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34437</th>\n",
       "      <td>911633318151544833</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @mashable: Beer company fills its cans with...</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id  Informativeness  \\\n",
       "43067  243372363088003072                1   \n",
       "11196  409458777738731520                1   \n",
       "2457   390774640202629120                0   \n",
       "55849  348128787155795971                0   \n",
       "34437  911633318151544833                1   \n",
       "\n",
       "                                                    text  positive_score  \\\n",
       "43067  BREAKING NEWS...\\n\\nA powerful 7.6-magnitude e...               1   \n",
       "11196  Funeral held for Kisook Ahn, one of four passe...               1   \n",
       "2457   Thank you to the thousands of firefighters who...               3   \n",
       "55849  PHOTO: The red circle is the train tunnel into...               1   \n",
       "34437  RT @mashable: Beer company fills its cans with...               1   \n",
       "\n",
       "       negative_score  emotional_devergence_score  \n",
       "43067              -1                         0.2  \n",
       "11196              -4                         0.5  \n",
       "2457               -3                         0.6  \n",
       "55849              -1                         0.2  \n",
       "34437              -3                         0.4  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Informativeness</th>\n",
       "      <th>text</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>emotional_devergence_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23069</th>\n",
       "      <td>218391134605545472</td>\n",
       "      <td>1</td>\n",
       "      <td>Pic: #CoFire: Helicopter drops water on #Waldo...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57851</th>\n",
       "      <td>1245917794815754240</td>\n",
       "      <td>1</td>\n",
       "      <td>#coronavirus cases surging to over 1 million w...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62987</th>\n",
       "      <td>541199590309912576</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @ABSCBNChannel2: #Hagupit approaches the Ph...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47132</th>\n",
       "      <td>466259645603274752</td>\n",
       "      <td>1</td>\n",
       "      <td>2 in Florida Show Symptoms of Deadly Middle Ea...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60378</th>\n",
       "      <td>347833028371968000</td>\n",
       "      <td>1</td>\n",
       "      <td>The town of #Sundre has declared a state of lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id  Informativeness  \\\n",
       "23069   218391134605545472                1   \n",
       "57851  1245917794815754240                1   \n",
       "62987   541199590309912576                1   \n",
       "47132   466259645603274752                1   \n",
       "60378   347833028371968000                1   \n",
       "\n",
       "                                                    text  positive_score  \\\n",
       "23069  Pic: #CoFire: Helicopter drops water on #Waldo...               1   \n",
       "57851  #coronavirus cases surging to over 1 million w...               1   \n",
       "62987  RT @ABSCBNChannel2: #Hagupit approaches the Ph...               2   \n",
       "47132  2 in Florida Show Symptoms of Deadly Middle Ea...               1   \n",
       "60378  The town of #Sundre has declared a state of lo...               1   \n",
       "\n",
       "       negative_score  emotional_devergence_score  \n",
       "23069              -1                         0.2  \n",
       "57851              -2                         0.3  \n",
       "62987              -1                         0.3  \n",
       "47132              -1                         0.2  \n",
       "60378              -2                         0.3  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    32332\n",
       "0    13497\n",
       "Name: Informativeness, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"Informativeness\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    13756\n",
       "0     5886\n",
       "Name: Informativeness, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"Informativeness\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_df.drop_duplicates(subset=[\"text\"], keep=\"first\", inplace=True)\n",
    "test_df.drop_duplicates(subset=[\"text\"], keep=\"first\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_processing(tweet):\n",
    "    \n",
    "    tweet= tweet.lower()\n",
    "    \n",
    "    #Removing hyperlinks from the tweet\n",
    "    tweet_no_links=re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    #Generating the list of words in the tweet (hashtags and other punctuations removed)\n",
    "    def form_sentence(tweet):\n",
    "        tweet_blob = TextBlob(tweet)\n",
    "        return ' '.join(tweet_blob.words)\n",
    "    new_tweet = form_sentence(tweet_no_links)\n",
    "    \n",
    "    #Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(tweet):\n",
    "        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    no_punc_tweet = no_user_alpha(new_tweet)\n",
    "    \n",
    "    #Normalizing the words in tweets \n",
    "    def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return \" \".join(normalized_tweet)\n",
    "    \n",
    "    \n",
    "    return normalization(no_punc_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_df['text_processed']=train_df['text'].apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_df['text_processed']=test_df['text'].apply(text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id                            348128787155795971                                                                                                 \n",
       "Informativeness               0                                                                                                                  \n",
       "text                          PHOTO: The red circle is the train tunnel into downtown #Calgary. #yycflood http://t.co/5zGG6LOBDD - @christiefaber\n",
       "positive_score                1                                                                                                                  \n",
       "negative_score                -1                                                                                                                 \n",
       "emotional_devergence_score    0.2                                                                                                                \n",
       "text_processed                photo red circle train tunnel downtown calgary yycflood christiefaber                                              \n",
       "Name: 55849, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "train_df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomise order of train dataset\n",
    "train_df = sklearn.utils.shuffle(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge with new data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors=pd.read_csv('./dataset_cleaning/tj/parsed/tweet_metadata.csv')\n",
    "users=pd.read_csv('./dataset_cleaning/tj/parsed/twitter_user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#Temporary\n",
    "\n",
    "\n",
    "datasets = [train_df, test_df]\n",
    "authors=pd.read_csv('./dataset_cleaning/tj/tweet_metadata_full.csv')\n",
    "users=pd.read_csv('./dataset_cleaning/tj/twitter_user_full.csv')\n",
    "authors.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "users.drop(columns=['Unnamed: 0','created_at', 'lang', 'name', 'screen_name', 'location','access'], inplace=True)\n",
    "users.columns=['author_id', 'has_description', 'bio_has_url', 'followers_count', 'friends_count',\n",
    "       'favourites_count', 'listed_count', 'statuses_count', 'protected',\n",
    "       'verified', 'default_profile', 'default_profile_image']\n",
    "\n",
    "\n",
    "mean=['retweet_count','favorite_count','followers_count', 'friends_count', 'listed_count', 'favourites_count', 'statuses_count']\n",
    "median = ['tweet_type', 'has_description', 'bio_has_url', 'protected', 'verified','default_profile', 'default_profile_image']\n",
    "\n",
    "\n",
    "for i,ds in enumerate(datasets):\n",
    "    print(\"Processing dataset {}\".format(i+1))\n",
    "    ds = pd.merge(ds, authors, on='id', how='left')\n",
    "\n",
    "    ds.drop(ds.columns.difference(['id', 'Informativeness', 'text_processed', 'author_id', 'tweet_type', 'retweet_count', 'favorite_count']), 1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    ds = pd.merge(ds, users, on='author_id', how='left')\n",
    "\n",
    "\n",
    "    for m in mean:\n",
    "        ds[m][ds['Informativeness'] == 1] = ds[m][ds['Informativeness'] == 1].fillna((ds[m][ds['Informativeness'] == 1].mean()))\n",
    "        ds[m][ds['Informativeness'] == 0] = ds[m][ds['Informativeness'] == 0].fillna((ds[m][ds['Informativeness'] == 0].mean()))\n",
    "\n",
    "    for m in median:\n",
    "        ds[m][ds['Informativeness'] == 1] = ds[m][ds['Informativeness'] == 1].fillna((ds[m][ds['Informativeness'] == 1].value_counts().idxmax()))\n",
    "        ds[m][ds['Informativeness'] == 0] = ds[m][ds['Informativeness'] == 0].fillna((ds[m][ds['Informativeness'] == 0].value_counts().idxmax()))\n",
    "\n",
    "    ds[\"has_description\"] = ds[\"has_description\"].apply(lambda x: 0 if x is np.nan else 1)\n",
    "    ds[\"tweet_type\"] = ds[\"tweet_type\"].apply(lambda x: 0 if x =='tweet' else 1)\n",
    "    ds[\"bio_has_url\"] = ds[\"bio_has_url\"].apply(lambda x: 0 if x is np.nan else 1)\n",
    "\n",
    "    ds = ds.drop(columns=['author_id'])\n",
    "\n",
    "    datasets[i] = ds\n",
    "train_df_extended =datasets[0]\n",
    "test_df_extended =datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_extended=pd.merge(train_df, authors, on='id')\n",
    "test_df_extended=pd.merge(test_df, authors, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_extended.drop(train_df_extended.columns.difference(['id', 'Informativeness', 'text_processed', 'author_id', 'tweet_type', 'retweet_count', 'favorite_count']), 1, inplace=True)\n",
    "test_df_extended.drop(test_df_extended.columns.difference(['id', 'Informativeness', 'text_processed', 'author_id', 'tweet_type', 'retweet_count', 'favorite_count']), 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.drop(columns=['created_at', 'lang', 'name', 'screen_name', 'location','access'], inplace=True)\n",
    "users.columns=['author_id', 'has_description', 'bio_has_url', 'followers_count', 'friends_count',\n",
    "       'favourites_count', 'listed_count', 'statuses_count', 'protected',\n",
    "       'verified', 'default_profile', 'default_profile_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_extended=pd.merge(train_df_extended, users, on='author_id')\n",
    "test_df_extended=pd.merge(test_df_extended, users, on='author_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_extended[\"has_description\"] = train_df_extended[\"has_description\"].apply(lambda x: 0 if x is np.nan else 1)\n",
    "test_df_extended[\"has_description\"] = test_df_extended[\"has_description\"].apply(lambda x: 0 if x is np.nan else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_extended[\"tweet_type\"] = train_df_extended[\"tweet_type\"].apply(lambda x: 0 if x =='tweet' else 1)\n",
    "test_df_extended[\"tweet_type\"] = test_df_extended[\"tweet_type\"].apply(lambda x: 0 if x == 'tweet' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_extended[\"bio_has_url\"] = train_df_extended[\"bio_has_url\"].apply(lambda x: 0 if x is np.nan else 1)\n",
    "test_df_extended[\"bio_has_url\"] = test_df_extended[\"bio_has_url\"].apply(lambda x: 0 if x is np.nan else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_extended = train_df_extended.drop(columns=['id', 'author_id'])\n",
    "test_df_extended = test_df_extended.drop(columns=['id', 'author_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into X and Y, as well as train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# sentences = main_df['text'].values\n",
    "# y = main_df['Informativeness'].values\n",
    "\n",
    "# sentences_train, sentences_test, y_train, y_test = train_test_split(\n",
    "#    sentences, y, test_size=0.25, random_state=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train = train_df['text_processed'].values\n",
    "sentences_test = test_df['text_processed'].values\n",
    "\n",
    "y_train = train_df['Informativeness'].values\n",
    "y_test = test_df['Informativeness'].values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pathological narcissist down st never take deaths chin admit direct responsibility gross negligence cause deaths absolutely wrong uk coronavirus test strategy unravel',\n",
       "       'nearly customers without power state hurricane sandy impact nj',\n",
       "       'fema epa gird irma house preps vote harvey aid', ...,\n",
       "       'usslakeerie service members assist srilanka devastate flood',\n",
       "       'pamnparsons sure question pam big yes',\n",
       "       'interest fact kuwait light coronavirus outbreak hundreds years ago male kuwaiti sailors return kuwait amp find go women amp children die due plague start bring new wive iran iraq etc'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Old way: Vectorize sentances using CountVectorizer  and do IDF \"manualy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Vevotize each sentance\n",
    "# vectorizer = CountVectorizer()\n",
    "# vectorizer.fit(sentences_train)\n",
    "\n",
    "# # Too look at the vocabulary encoding\n",
    "# # vectorizer.vocabulary_\n",
    "\n",
    "# X_train = vectorizer.transform(sentences_train)\n",
    "# X_test  = vectorizer.transform(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# tfidf_transformer.fit(X_train)\n",
    "# X_train_tfidf = tfidf_transformer.transform(X_train)\n",
    "# X_test_tfidf = tfidf_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df_extended.drop(columns=['Informativeness'])\n",
    "X_test = test_df_extended.drop(columns=['Informativeness'])\n",
    "\n",
    "\n",
    "y_train = train_df_extended['Informativeness']\n",
    "y_test = test_df_extended['Informativeness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text=X_train['text_processed']\n",
    "X_train_numerical= X_train.drop(columns=[\"text_processed\"])\n",
    "\n",
    "X_test_text=X_test['text_processed']\n",
    "X_test_numerical= X_test.drop(columns=[\"text_processed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "def grid_search_text_and_numerical(model, parameters, X_train_text, X_train_numerical):\n",
    "    best_score=0\n",
    "    best_params={}\n",
    "    best_clf= None\n",
    "    best_vectorizer = None\n",
    "    best_tfIdfTransformer=None\n",
    "    for ngram_range in [(1, 1), (1, 2)]:\n",
    "        for use_tf_idf in [True, False]:\n",
    "            for normalization_norm in [\"max\", \"l2\", \"none\"]:\n",
    "\n",
    "                print(\"Iteration with {}, {}, {}\".format(ngram_range, use_tf_idf, normalization_norm))\n",
    "                vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "                vectorizer.fit(X_train_text)\n",
    "                X_train_text_vectorized = vectorizer.transform(X_train_text)\n",
    "\n",
    "                tfidf_transformer = TfidfTransformer(use_idf = use_tf_idf)\n",
    "                tfidf_transformer.fit(X_train_text_vectorized)\n",
    "                X_train_text_tfidf = tfidf_transformer.transform(X_train_text_vectorized)\n",
    "\n",
    "\n",
    "                X_train_merged = hstack((X_train_text_tfidf,np.array(np.array(X_train_numerical).astype(np.float))))\n",
    "\n",
    "                if(normalization_norm!=\"none\"):\n",
    "                    X_train_merged = normalize(X_train_merged, norm=normalization_norm, axis=0)\n",
    "    #             X_train_merged = scale(X_train_merged, with_mean= False, axis = 0)\n",
    "\n",
    "                clf = GridSearchCV(model, parameters, scoring='roc_auc', n_jobs=-1, verbose=2)\n",
    "                clf.fit(X_train_merged, y_train)\n",
    "\n",
    "                if(best_score < clf.best_score_):\n",
    "                    best_score = clf.best_score_\n",
    "                    best_params = clf.best_params_\n",
    "                    best_params[\"ngram_range\"] = ngram_range\n",
    "                    best_params[\"use_tf_idf\"]= use_tf_idf\n",
    "                    best_params[\"normalization_norm\"] = normalization_norm\n",
    "                    best_clf=clf\n",
    "\n",
    "                    best_vectorizer = vectorizer\n",
    "                    best_tfIdfTransformer=tfidf_transformer\n",
    "                \n",
    "    return best_score, best_params, best_clf, best_vectorizer, best_tfIdfTransformer\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), True, max\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   11.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), True, l2\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), True, none\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), False, max\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    6.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), False, l2\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 1), False, none\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), True, max\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   39.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), True, l2\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   14.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), True, none\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), False, max\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   38.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), False, l2\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   14.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration with (1, 2), False, none\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score found while training: 0.8980010811308168\n",
      "{'penalty': 'l2', 'ngram_range': (1, 2), 'use_tf_idf': False, 'normalization_norm': 'max'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_parameters = {'penalty':('l2',)}\n",
    "lr_model = LogisticRegression( max_iter=1000)\n",
    "\n",
    "lr={}\n",
    "lr[\"model\"]=lr_model\n",
    "lr[\"parameters\"] =lr_parameters\n",
    "\n",
    "lr_best_score, lr_best_params, lr_best_clf, lr_best_vectorizer, lr_best_tfIdfTransformer = grid_search_text_and_numerical(lr[\"model\"], lr[\"parameters\"], X_train_text, X_train_numerical)\n",
    "    \n",
    "print(\"Best score found while training: {}\".format(lr_best_score))\n",
    "print(lr_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test_and_score(X_test_text, X_test_numerical, best_vecotorizer, best_tfIdfTransformer, best_params, best_clf):\n",
    "\n",
    "    X_test_text_vectorized = best_vecotorizer.transform(X_test_text)\n",
    "    X_test_text_tfidf = best_tfIdfTransformer.transform(X_test_text_vectorized)\n",
    "\n",
    "\n",
    "    X_test_merged = hstack((X_test_text_tfidf,np.array(np.array(X_test_numerical).astype(np.float))))\n",
    "    \n",
    "    if(best_params[\"normalization_norm\"]!=\"none\"):\n",
    "        X_test_merged = normalize(X_test_merged, norm=best_params[\"normalization_norm\"], axis=0)\n",
    "    # X_test_merged = scale(X_test_merged, with_mean= False, axis = 0)\n",
    "\n",
    "    return best_clf.score(X_test_merged, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8988659144450964"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_test_and_score(X_test_text, X_test_numerical, lr_best_vectorizer, lr_best_tfIdfTransformer, lr_best_params, lr_best_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.8874170859369963\n",
      "Best parameters:  {'clf-lr__penalty': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "text_clf_lr = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-lr', LogisticRegression( max_iter=1000))])\n",
    "\n",
    "parameters_lr = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),'clf-lr__penalty': ('l2', 'l1')}\n",
    "\n",
    "gs_clf_lr = GridSearchCV(text_clf_lr, parameters_lr ,scoring='roc_auc', n_jobs=-1, verbose=2)\n",
    "gs_clf_lr = gs_clf_lr.fit(X_train_text, y_train)\n",
    "\n",
    "\n",
    "print(\"Best score: \", gs_clf_lr.best_score_)\n",
    "print(\"Best parameters: \", gs_clf_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7076308778130103"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf_lr.score(X_test_text, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.645826025955805 \n",
      "Recall:0.6338209982788297 \n",
      "F1 Score: 0.6397671994440585 \n",
      "AUC: 0.5016850207165987\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs_clf_lr.predict(sentences_test)\n",
    "y_pred_prob=gs_clf_lr.predict_proba(sentences_test)[:,1]\n",
    "\n",
    "precision_LR = precision_score(y_test, y_pred)\n",
    "recall_LR = recall_score(y_test, y_pred)\n",
    "f1_LR= f1_score(y_test, y_pred)\n",
    "roc_LR = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "fpr_LR, tpr_LR, thresholds_LR = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "print(\"Precision: {} \\nRecall:{} \\nF1 Score: {} \\nAUC: {}\".format(precision_LR, recall_LR, f1_LR, roc_LR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [47256, 34825]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-bb6a784beb00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mgs_clf_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_clf_svm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters_svm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mgs_clf_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_clf_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \"\"\"\n\u001b[1;32m    291\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 256\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [47256, 34825]"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier( random_state=42, early_stopping=True))])\n",
    "\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False),\n",
    "                  'clf-svm__loss':('hinge','log',),'clf-svm__max_iter': (100,1000),\n",
    "                 'clf-svm__alpha':(1e-3,1e-6, 1e-10),'clf-svm__penalty':('l2',),\n",
    "                 }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm,scoring='roc_auc', n_jobs=-1, verbose=10)\n",
    "gs_clf_svm = gs_clf_svm.fit(sentences_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best score: \", gs_clf_svm.best_score_)\n",
    "print(\"Best parameters: \", gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = gs_clf_svm.best_estimator_[0].get_feature_names() \n",
    "coefs = gs_clf_svm.best_estimator_[-1].coef_[0]\n",
    "\n",
    "coefs_with_fns = sorted(zip(coefs, feature_names)) \n",
    "df=pd.DataFrame(coefs_with_fns)\n",
    "df.columns='coefficient','word'\n",
    "df.sort_values(by='coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_svm.score(sentences_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs_clf_svm.predict(sentences_test)\n",
    "y_pred_prob=gs_clf_svm.predict_proba(sentences_test)[:,1]\n",
    "\n",
    "precision_SVM = precision_score(y_test, y_pred)\n",
    "recall_SVM = recall_score(y_test, y_pred)\n",
    "f1_SVM= f1_score(y_test, y_pred)\n",
    "roc_SVM = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "fpr_SVM, tpr_SVM, thresholds_SVM = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "print(\"Precision: {} \\nRecall:{} \\nF1 Score: {} \\nAUC: {}\".format(precision_SVM, recall_SVM, f1_SVM, roc_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "text_clf_nb = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-nb', MultinomialNB())])\n",
    "\n",
    "parameters_nb = {'vect__ngram_range': [(1, 1), (1, 2),(2,2)], 'tfidf__use_idf': (True, False),\n",
    "                  'clf-nb__alpha': (1,1e-1, 1e-3)}\n",
    "\n",
    "gs_clf_nb = GridSearchCV(text_clf_nb, parameters_nb,scoring='roc_auc', n_jobs=-1, verbose=5)\n",
    "gs_clf_nb = gs_clf_nb.fit(sentences_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best score: \", gs_clf_nb.best_score_)\n",
    "print(\"Best parameters: \", gs_clf_nb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_nb.score(sentences_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs_clf_nb.predict(sentences_test)\n",
    "y_pred_prob=gs_clf_nb.predict_proba(sentences_test)[:,1]\n",
    "\n",
    "accuracy_NB = accuracy_score(y_test, y_pred)\n",
    "precision_NB = precision_score(y_test, y_pred)\n",
    "recall_NB = recall_score(y_test, y_pred)\n",
    "f1_NB= f1_score(y_test, y_pred)\n",
    "roc_NB = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "fpr_NB, tpr_NB, thresholds_NB = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "print(\"Accuracy: {} \\nPrecision: {} \\nRecall:{} \\nF1 Score: {} \\nAUC: {}\".format(accuracy_NB,precision_NB, recall_NB, f1_NB, roc_NB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "text_clf_dt = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-dt', DecisionTreeClassifier(  ))])\n",
    "\n",
    "parameters_dt = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, ),\n",
    "                'clf-dt__max_depth':[200, 400, 1000]}\n",
    "\n",
    "gs_clf_dt = GridSearchCV(text_clf_dt, parameters_dt,scoring='roc_auc', n_jobs=-1, verbose=5)\n",
    "gs_clf_dt = gs_clf_dt.fit(sentences_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best score: \", gs_clf_dt.best_score_)\n",
    "print(\"Best parameters: \", gs_clf_dt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_dt.score(sentences_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs_clf_dt.predict(sentences_test)\n",
    "y_pred_prob=gs_clf_dt.predict_proba(sentences_test)[:,1]\n",
    "\n",
    "accuracy_DT = accuracy_score(y_test, y_pred)\n",
    "precision_DT = precision_score(y_test, y_pred)\n",
    "recall_DT = recall_score(y_test, y_pred)\n",
    "f1_DT= f1_score(y_test, y_pred)\n",
    "roc_DT = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "fpr_DT, tpr_DT, thresholds_DT = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "print(\"Accuracy: {} \\nPrecision: {} \\nRecall:{} \\nF1 Score: {} \\nAUC: {}\".format(accuracy_DT,precision_DT, recall_DT, f1_DT, roc_DT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "text_clf_rf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-rf', RandomForestClassifier( ))])\n",
    "\n",
    "parameters_rf = {'vect__ngram_range': [(1, 1),(2,2)], 'tfidf__use_idf': (True, False),\n",
    "                'clf-rf__n_estimators':[800,], 'clf-rf__min_samples_split':[4,10]}\n",
    "\n",
    "gs_clf_rf = GridSearchCV(text_clf_rf, parameters_rf, scoring='roc_auc', n_jobs=-1, verbose=10)\n",
    "gs_clf_rf = gs_clf_rf.fit(sentences_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Best score: \", gs_clf_rf.best_score_)\n",
    "print(\"Best parameters: \", gs_clf_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf_rf.score(sentences_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using embeddings (Glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photos deadly wildfires rage california\n",
      "[290, 103, 90, 4049, 13]\n"
     ]
    }
   ],
   "source": [
    "# This time don't vectorize, but tokenize. Same idea but runs better with Keras and tokenizer assings\n",
    "# index 1 to most frequent word and so on. While vecotrizer makes each sentcance to a vector of the same\n",
    "# size (the vocabulary size) and assigns count of how many time a word appears\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(sentences_train[1])\n",
    "print(X_train[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   90 1764  108   13 1349    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Pad with trailing 0 every tokenized sentance so that they have the same length (the length of the longest sentance)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = len(max(X_train, key=len)) #length of the longest sentance\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download here: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "embedding_dim = 300\n",
    "embedding_matrix = create_embedding_matrix(\n",
    "    'glove_data/glove.6B.300d.txt',\n",
    "    tokenizer.word_index, embedding_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percantage of vocabulary covered in the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5496727877607301"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 415, 300)          13706700  \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                3010      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 13,709,721\n",
      "Trainable params: 3,021\n",
      "Non-trainable params: 13,706,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import keras\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=False))\n",
    "\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[keras.metrics.AUC()])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4726/4726 [==============================] - 20s 4ms/step - loss: 0.4433 - auc: 0.8244 - val_loss: 0.6166 - val_auc: 0.7189\n",
      "Epoch 2/30\n",
      "4726/4726 [==============================] - 20s 4ms/step - loss: 0.4144 - auc: 0.8516 - val_loss: 0.6862 - val_auc: 0.6970\n",
      "Epoch 3/30\n",
      "4726/4726 [==============================] - 15s 3ms/step - loss: 0.4057 - auc: 0.8581 - val_loss: 0.6316 - val_auc: 0.7074\n",
      "Epoch 4/30\n",
      "4726/4726 [==============================] - 18s 4ms/step - loss: 0.4005 - auc: 0.8619 - val_loss: 0.6191 - val_auc: 0.7127\n",
      "Epoch 5/30\n",
      "4726/4726 [==============================] - 18s 4ms/step - loss: 0.3962 - auc: 0.8653 - val_loss: 0.6580 - val_auc: 0.7074\n",
      "Epoch 6/30\n",
      "4726/4726 [==============================] - 17s 4ms/step - loss: 0.3937 - auc: 0.8672 - val_loss: 0.6292 - val_auc: 0.7147\n",
      "Epoch 7/30\n",
      "4726/4726 [==============================] - 18s 4ms/step - loss: 0.3900 - auc: 0.8701 - val_loss: 0.6247 - val_auc: 0.7100\n",
      "Epoch 8/30\n",
      "4726/4726 [==============================] - 19s 4ms/step - loss: 0.3881 - auc: 0.8713 - val_loss: 0.6364 - val_auc: 0.7108\n",
      "Epoch 9/30\n",
      "4726/4726 [==============================] - 23s 5ms/step - loss: 0.3851 - auc: 0.8735 - val_loss: 0.6314 - val_auc: 0.7111\n",
      "Epoch 10/30\n",
      "4726/4726 [==============================] - 23s 5ms/step - loss: 0.3838 - auc: 0.8744 - val_loss: 0.6883 - val_auc: 0.6929\n",
      "Epoch 11/30\n",
      "4726/4726 [==============================] - 18s 4ms/step - loss: 0.3815 - auc: 0.8763 - val_loss: 0.6478 - val_auc: 0.7015\n",
      "Epoch 12/30\n",
      "4726/4726 [==============================] - 20s 4ms/step - loss: 0.3800 - auc: 0.8771 - val_loss: 0.6517 - val_auc: 0.7088\n",
      "Epoch 13/30\n",
      "4726/4726 [==============================] - 19s 4ms/step - loss: 0.3783 - auc: 0.8782 - val_loss: 0.6563 - val_auc: 0.7032\n",
      "Epoch 14/30\n",
      "4726/4726 [==============================] - 19s 4ms/step - loss: 0.3769 - auc: 0.8793 - val_loss: 0.6643 - val_auc: 0.6982\n",
      "Epoch 15/30\n",
      "4726/4726 [==============================] - 23s 5ms/step - loss: 0.3746 - auc: 0.8811 - val_loss: 0.6595 - val_auc: 0.7098\n",
      "Epoch 16/30\n",
      "4726/4726 [==============================] - 22s 5ms/step - loss: 0.3737 - auc: 0.8817 - val_loss: 0.6413 - val_auc: 0.7153\n",
      "Epoch 17/30\n",
      "4726/4726 [==============================] - 18s 4ms/step - loss: 0.3723 - auc: 0.8827 - val_loss: 0.6546 - val_auc: 0.7234\n",
      "Epoch 18/30\n",
      "4726/4726 [==============================] - 20s 4ms/step - loss: 0.3713 - auc: 0.8833 - val_loss: 0.6689 - val_auc: 0.7021\n",
      "Epoch 19/30\n",
      "4726/4726 [==============================] - 22s 5ms/step - loss: 0.3694 - auc: 0.8845 - val_loss: 0.6739 - val_auc: 0.7058\n",
      "Epoch 20/30\n",
      "4726/4726 [==============================] - 21s 4ms/step - loss: 0.3683 - auc: 0.8855 - val_loss: 0.6503 - val_auc: 0.7166\n",
      "Epoch 21/30\n",
      "4726/4726 [==============================] - 23s 5ms/step - loss: 0.3668 - auc: 0.8861 - val_loss: 0.6803 - val_auc: 0.7041\n",
      "Epoch 22/30\n",
      "4726/4726 [==============================] - 21s 4ms/step - loss: 0.3659 - auc: 0.8866 - val_loss: 0.6667 - val_auc: 0.7074\n",
      "Epoch 23/30\n",
      "4726/4726 [==============================] - 19s 4ms/step - loss: 0.3652 - auc: 0.8872 - val_loss: 0.6768 - val_auc: 0.7073\n",
      "Epoch 24/30\n",
      "4726/4726 [==============================] - 23s 5ms/step - loss: 0.3641 - auc: 0.8876 - val_loss: 0.6891 - val_auc: 0.7022\n",
      "Epoch 25/30\n",
      "4726/4726 [==============================] - 19s 4ms/step - loss: 0.3633 - auc: 0.8881 - val_loss: 0.7113 - val_auc: 0.6898\n",
      "Epoch 26/30\n",
      "4726/4726 [==============================] - 18s 4ms/step - loss: 0.3617 - auc: 0.8892 - val_loss: 0.7195 - val_auc: 0.6979\n",
      "Epoch 27/30\n",
      "4726/4726 [==============================] - 21s 5ms/step - loss: 0.3606 - auc: 0.8899 - val_loss: 0.7002 - val_auc: 0.7082\n",
      "Epoch 28/30\n",
      "4726/4726 [==============================] - 19s 4ms/step - loss: 0.3608 - auc: 0.8898 - val_loss: 0.6685 - val_auc: 0.7199\n",
      "Epoch 29/30\n",
      "4726/4726 [==============================] - 19s 4ms/step - loss: 0.3589 - auc: 0.8911 - val_loss: 0.6627 - val_auc: 0.7097\n",
      "Epoch 30/30\n",
      "4726/4726 [==============================] - 18s 4ms/step - loss: 0.3589 - auc: 0.8911 - val_loss: 0.6844 - val_auc: 0.7147\n",
      "Training Accuracy: 0.895510196685791\n",
      "Testing Accuracy:  0.7146984934806824\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {}\".format(accuracy))\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.720785786508739 \n",
      "Recall:0.8588640275387264 \n",
      "F1 Score: 0.7837901515746487 \n",
      "AUC: 0.7147341958246786\n"
     ]
    }
   ],
   "source": [
    "y_pred =(model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "precision_Glove_Untrainable = precision_score(y_test, y_pred)\n",
    "recall_Glove_Untrainable = recall_score(y_test, y_pred)\n",
    "f1_Glove_Untrainable= f1_score(y_test, y_pred)\n",
    "roc_Glove_Untrainable = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "fpr_Glove_Untrainable, tpr_Glove_Untrainable, thresholds_Glove_Untrainable = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "print(\"Precision: {} \\nRecall:{} \\nF1 Score: {} \\nAUC: {}\".format(precision_Glove_Untrainable, recall_Glove_Untrainable, f1_Glove_Untrainable, roc_Glove_Untrainable))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_keras_history import plot_history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plot_history(history.history)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same thing with glove, but this time all parameters are trainable (takes much longer to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=True))\n",
    "\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {}\".format(accuracy))\n",
    "plot_history(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =(model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "precision_Glove = precision_score(y_test, y_pred)\n",
    "recall_Glove = recall_score(y_test, y_pred)\n",
    "f1_Glove = f1_score(y_test, y_pred)\n",
    "roc_Glove = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "fpr_Glove, tpr_Glove, thresholds_Glove= roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "print(\"Precision: {} \\nRecall:{} \\nF1 Score: {} \\nAUC: {}\".format(precision_Glove, recall_Glove, f1_Glove, roc_Glove))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_Glove, tpr_Glove, label='Glove Trainable (area = {:.3f})'.format(roc_Glove))\n",
    "plt.plot(fpr_Glove_Untrainable, tpr_Glove_Untrainable, label='Glove Untrainable (area = {:.3f})'.format(roc_Glove_Untrainable))\n",
    "plt.plot(fpr_LR, tpr_LR, label='Logistic Regression (area = {:.3f})'.format(roc_LR))\n",
    "# plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "# Zoom in view of the upper left corner.\n",
    "# plt.figure(2)\n",
    "# plt.xlim(0, 0.2)\n",
    "# plt.ylim(0.8, 1)\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "# # plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "# plt.xlabel('False positive rate')\n",
    "# plt.ylabel('True positive rate')\n",
    "# plt.title('ROC curve (zoomed in at top left)')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simpletransformers\n",
    "from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel('roberta', 'roberta-base', use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNew=pd.DataFrame({ 'text':sentences_train,'labels':y_train})\n",
    "testNew=pd.DataFrame({ 'text':sentences_test,'labels':y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_model(trainNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(['dead pray for victim Some arbitary sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "\n",
    "result, model_outputs, wrong_predictions = model.eval_model(testNew, acc=sklearn.metrics.accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO1PgzWEyCJpMQ1B3Nl/qH0",
   "collapsed_sections": [],
   "name": "glove.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
