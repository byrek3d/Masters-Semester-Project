{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import glob\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train features: 57348 \n",
      "Number of test features: 7929\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob(\"./dataset_cleaning/*.pkl\")\n",
    "all_files.sort()\n",
    "test_filename= \"./dataset_cleaning/dataset4Cleaned.pkl\"\n",
    "test2_filename= \"./dataset_cleaning/dataset7Cleaned.pkl\"\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    \n",
    "    if(filename != test_filename and filename != test2_filename):\n",
    "\n",
    "        df = pd.read_pickle(filename)\n",
    "        li.append(df)\n",
    "\n",
    "train = pd.concat(li, axis=0, ignore_index=True)\n",
    "test = pd.read_pickle(test_filename)\n",
    "\n",
    "print(\"Number of train features: {} \\nNumber of test features: {}\".format(len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Informativeness', 'text', 'positive_score', 'negative_score',\n",
       "       'emotional_devergence_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "authors=pd.read_csv('./dataset_cleaning/tj/parsed/tweet_metadata.csv')\n",
    "users=pd.read_csv('./dataset_cleaning/tj/parsed/twitter_user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.merge(train, authors, on='id')\n",
    "test=pd.merge(test, authors, on='id')\n",
    "train.drop(train.columns.difference(['id', 'Informativeness', 'text', 'author_id', 'tweet_type','positive_score', 'negative_score',\n",
    "       'emotional_devergence_score' ]), 1, inplace=True)\n",
    "test.drop(test.columns.difference(['id', 'Informativeness', 'text', 'author_id', 'tweet_type','positive_score', 'negative_score',\n",
    "       'emotional_devergence_score' ]), 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.drop(columns=['created_at', 'lang', 'name', 'screen_name', 'location','access'], inplace=True)\n",
    "users.columns=['author_id', 'has_description', 'bio_has_url', 'followers_count', 'friends_count',\n",
    "       'favourites_count', 'listed_count', 'statuses_count', 'protected',\n",
    "       'verified', 'default_profile', 'default_profile_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.merge(train, users, on='author_id')\n",
    "test=pd.merge(test, users, on='author_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Informativeness</th>\n",
       "      <th>text</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>emotional_devergence_score</th>\n",
       "      <th>author_id</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>has_description</th>\n",
       "      <th>bio_has_url</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>protected</th>\n",
       "      <th>verified</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>211040709124440064</td>\n",
       "      <td>0</td>\n",
       "      <td>#Intern #US #TATTOO #Wisconsin #Ohio #NC #PA #...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>601864285</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Traveling http://goo.gl/97yT2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>402</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>210864180218167296</td>\n",
       "      <td>0</td>\n",
       "      <td>Get in on the fun every Thursday with the @csi...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>245545247</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Director of Marketing &amp; Promotions for the Col...</td>\n",
       "      <td>http://www.csindy.com/colorado/Home</td>\n",
       "      <td>454</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>3327</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>211157222699433985</td>\n",
       "      <td>0</td>\n",
       "      <td>Welcome to our newest STUDENTathlete- Reagan B...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>487854217</td>\n",
       "      <td>tweet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>422</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>211162553659830272</td>\n",
       "      <td>0</td>\n",
       "      <td>Denver Post: #Colorado governor signs bill cre...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>17602802</td>\n",
       "      <td>tweet</td>\n",
       "      <td>The Reporters Committee for Freedom of the Pre...</td>\n",
       "      <td>http://www.rcfp.org</td>\n",
       "      <td>1965</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>4392</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>211216962162933761</td>\n",
       "      <td>0</td>\n",
       "      <td>Pretty sure I'm going to live in Manitou Sprin...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>348495572</td>\n",
       "      <td>tweet</td>\n",
       "      <td>free-spirited, ever thinking, world citizen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>227</td>\n",
       "      <td>203</td>\n",
       "      <td>23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3687</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id  Informativeness  \\\n",
       "0  211040709124440064                0   \n",
       "1  210864180218167296                0   \n",
       "2  211157222699433985                0   \n",
       "3  211162553659830272                0   \n",
       "4  211216962162933761                0   \n",
       "\n",
       "                                                text  positive_score  \\\n",
       "0  #Intern #US #TATTOO #Wisconsin #Ohio #NC #PA #...               1   \n",
       "1  Get in on the fun every Thursday with the @csi...               2   \n",
       "2  Welcome to our newest STUDENTathlete- Reagan B...               2   \n",
       "3  Denver Post: #Colorado governor signs bill cre...               1   \n",
       "4  Pretty sure I'm going to live in Manitou Sprin...               3   \n",
       "\n",
       "   negative_score  emotional_devergence_score  author_id tweet_type  \\\n",
       "0              -1                         0.2  601864285      tweet   \n",
       "1              -1                         0.3  245545247      tweet   \n",
       "2              -1                         0.3  487854217      tweet   \n",
       "3              -1                         0.2   17602802      tweet   \n",
       "4              -1                         0.4  348495572      tweet   \n",
       "\n",
       "                                     has_description  \\\n",
       "0                      Traveling http://goo.gl/97yT2   \n",
       "1  Director of Marketing & Promotions for the Col...   \n",
       "2                                                NaN   \n",
       "3  The Reporters Committee for Freedom of the Pre...   \n",
       "4        free-spirited, ever thinking, world citizen   \n",
       "\n",
       "                           bio_has_url  followers_count  friends_count  \\\n",
       "0                                  NaN               62             19   \n",
       "1  http://www.csindy.com/colorado/Home              454             35   \n",
       "2                                  NaN                1              1   \n",
       "3                  http://www.rcfp.org             1965            188   \n",
       "4                                  NaN              227            203   \n",
       "\n",
       "   favourites_count  listed_count  statuses_count  protected  verified  \\\n",
       "0                 0           0.0             402      False     False   \n",
       "1                 2          28.0            3327      False     False   \n",
       "2                 0           0.0             422      False     False   \n",
       "3                 0         181.0            4392      False     False   \n",
       "4                23           2.0            3687      False     False   \n",
       "\n",
       "   default_profile  default_profile_image  \n",
       "0             True                   True  \n",
       "1            False                  False  \n",
       "2             True                   True  \n",
       "3            False                  False  \n",
       "4             True                  False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_processing(tweet):\n",
    "    \n",
    "    tweet= tweet.lower()\n",
    "    \n",
    "    #Removing hyperlinks from the tweet\n",
    "    tweet_no_links=re.sub(r'http\\S+', '', tweet)\n",
    "    \n",
    "    #Generating the list of words in the tweet (hashtags and other punctuations removed)\n",
    "    def form_sentence(tweet):\n",
    "        tweet_blob = TextBlob(tweet)\n",
    "        return ' '.join(tweet_blob.words)\n",
    "    new_tweet = form_sentence(tweet_no_links)\n",
    "    \n",
    "    #Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(tweet):\n",
    "        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    no_punc_tweet = no_user_alpha(new_tweet)\n",
    "    \n",
    "    #Normalizing the words in tweets \n",
    "    def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return \" \".join(normalized_tweet)\n",
    "    \n",
    "    \n",
    "    return normalization(no_punc_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_hashtags(text):\n",
    "    return text.count('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_present(text):\n",
    "    return int('http' in text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def text_has_emoji(text):\n",
    "    return any(map(text[:-50].__contains__, [':)',':(',':P',':c',':<','c:','<:',':L',':l','^_^','^.^','>_<','>.<','>_>','<_<','>.>','<.<','-.-','-_-','o_o','o.o','._.','owo','OwO',';_;','>:)',':]',':}','>:(','>:|','-.^','-_^','8)','B)','<3','xD',':3','x3','\\o','\\o/',';_;','OwO','uwu','O:)',':#',':*']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_instructional_words(text):\n",
    "    return any(map(text.__contains__, [\"text\", 'call', 'donate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_phone_number(text):\n",
    "    return bool(re.search('\\\\d{3}-\\\\d{3}-\\\\d{4}', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_has_slang(text):\n",
    "    for i in text[:-50].split(' '):\n",
    "        for j in ['afaik','afk','asl','atm','atw','ayy','bae','bb','bbiab','bbl','bbs','bc','bf','bff','bork','brb','btw','cba','convo','cp','cya','cya','dank','dc','dem','dw','e2e','fml','FOMO','FTFY','ftl','ftw','fwiw','fyi','g2g','g4u','gf','gg','goml','gr8','gratz','gtfo','guiz','hbu','hru','ianadb','ianalb','ianap','idc','idgaf','idk','iirc','ik','ikr','ily','inb4','irl','jfc','jk','js','k','kappa','kek','kms','kthx','l8r','leet','lmao','lmk','lol','LPT','lrl','lrn2','m8','maga','mfw','mrw','nerf','ngl','nm','nmu','noob','nvm','ofc','omf','omg','omw','ooc','op','OP','orly','pepe','pleb','pleb','plz','pron','pwned','REEEEEE','rekt','rickrol','rip','rly','rms','rofl','rotflol','rtfm','rude','shank','smd','smh','soz','swag','tbf','tbh','tbt','TIFU','tf','tfw','thx','tide','TIL','tl;dr','tmw','tolo','topkek','ty','uwotm8','w00t','wb','wot','wtb','wtf','wtg','wts','wuu2','yarly','ymmv','yolo','yw']:\n",
    "            if(i==j):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_RT(text):\n",
    "    for i in text[:-50].split(' '):\n",
    "        if(i == 'RT'):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_profanity(text):\n",
    "    for i in text[:-50].split(' '):\n",
    "        for j in ['acrotomophilia','anal','anilingus','anus','arsehole','ass','asshole','assmunch','auto erotic','autoerotic','babeland','baby batter','ball gag','ball gravy','ball kicking','ball licking','ball sack','ball sucking','bangbros','bareback','barely legal','barenaked','bastardo','bastinado','bbw','bdsm','beaver cleaver','beaver lips','bestiality','bi curious','big black','big breasts','big knockers','big tits','bimbos','birdlock','bitch','black cock','blonde action','blonde on blonde action','blow j','blow your l','blue waffle','blumpkin','bollocks','bondage','boner','boob','boobs','booty call','brown showers','brunette action','bukkake','bulldyke','bullet vibe','bung hole','bunghole','busty','butt','buttcheeks','butthole','camel toe','camgirl','camslut','camwhore','carpet muncher','carpetmuncher','chocolate rosebuds','circlejerk','cleveland steamer','clit','clitoris','clover clamps','clusterfuck','cock','cocks','coprolagnia','coprophilia','cornhole','cum','cumming','cunnilingus','cunt','darkie','date rape','daterape','deep throat','deepthroat','dick','dildo','dirty pillows','dirty sanchez','dog style','doggie style','doggiestyle','doggy style','doggystyle','dolcett','domination','dominatrix','dommes','donkey punch','double dong','double penetration','dp action','eat my ass','ecchi','ejaculation','erotic','erotism','escort','ethical slut','eunuch','faggot','fecal','felch','fellatio','feltch','female squirting','femdom','figging','fingering','fisting','foot fetish','footjob','frotting','fuck','fucking','fuck buttons','fudge packer','fudgepacker','futanari','g-spot','gang bang','gay sex','genitals','giant cock','girl on','girl on top','girls gone wild','goatcx','goatse','gokkun','golden shower','goo girl','goodpoop','goregasm','grope','group sex','guro','hand job','handjob','hard core','hardcore','hentai','homoerotic','honkey','hooker','hot chick','how to kill','how to murder','huge fat','humping','incest','intercourse','jack off','jail bait','jailbait','jerk off','jigaboo','jiggaboo','jiggerboo','jizz','juggs','kike','kinbaku','kinkster','kinky','knobbing','leather restraint','leather straight jacket','lemon party','lolita','lovemaking','make me come','male squirting','masturbate','menage a trois','milf','missionary position','motherfucker','mound of venus','mr hands','muff diver','muffdiving','nambla','nawashi','negro','neonazi','nig nog','nigga','nigger','nimphomania','nipple','nipples','nsfw images','nude','nudity','nympho','nymphomania','octopussy','omorashi','one cup two girls','one guy one jar','orgasm','orgy','paedophile','panties','panty','pedobear','pedophile','pegging','penis','phone sex','piece of shit','piss pig','pissing','pisspig','playboy','pleasure chest','pole smoker','ponyplay','poof','poop chute','poopchute','porn','porno','pornography','prince albert piercing','pthc','pubes','pussy','queaf','raghead','raging boner','rape','raping','rapist','rectum','reverse cowgirl','rimjob','rimming','rosy palm','rosy palm and her 5 sisters','rusty trombone','s&m','sadism','scat','schlong','scissoring','semen','sex','sexo','sexy','shaved beaver','shaved pussy','shemale','shibari','shit','shota','shrimping','slanteye','slut','smut','snatch','snowballing','sodomize','sodomy','spic','spooge','spread legs','strap on','strapon','strappado','strip club','style doggy','suck','sucks','suicide girls','sultry women','swastika','swinger','tainted love','taste my','tea bagging','threesome','throating','tied up','tight white','tit','tits','titties','titty','tongue in a','topless','tosser','towelhead','tranny','tribadism','tub girl','tubgirl','tushy','twat','twink','twinkie','two girls one cup','undressing','upskirt','urethra play','urophilia','vagina','venus mound','vibrator','violet blue','violet wand','vorarephilia','voyeur','vulva','wank','wet dream','wetback','white power','women rapping','wrapping men','wrinkled starfish','xx','xxx','yaoi','yellow showers','yiffy','zoophilia']:\n",
    "            if(i==j):\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_word_sentance(text):\n",
    "    return int(len(text.split(\" \"))<=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"hashtag_count\"]=train['text'].apply(count_hashtags)\n",
    "test[\"hashtag_count\"]=test['text'].apply(count_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"hashtag_present\"] = train[\"hashtag_count\"].apply(lambda x: np.sign(x))\n",
    "test[\"hashtag_present\"] = test[\"hashtag_count\"].apply(lambda x: np.sign(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"link_present\"]=train['text'].apply(link_present)\n",
    "test[\"link_present\"]=test['text'].apply(link_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"emoji_present\"] = train['text'].apply(text_has_emoji)\n",
    "test[\"emoji_present\"] = test['text'].apply(text_has_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"instructinal_keyword\"] = train['text'].apply(has_instructional_words)\n",
    "test[\"instructinal_keyword\"] = test['text'].apply(has_instructional_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"contains_phone_number\"] = train['text'].apply(has_phone_number)\n",
    "test[\"contains_phone_number\"] = test['text'].apply(has_phone_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"slang_present\"] = train['text'].apply(text_has_slang)\n",
    "test[\"slang_present\"] = test['text'].apply(text_has_slang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"is_RT\"] = train['text'].apply(is_RT)\n",
    "test[\"is_RT\"] = test['text'].apply(is_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"has_profanity\"] = train['text'].apply(has_profanity)\n",
    "test[\"has_profanity\"] = test['text'].apply(has_profanity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"one_word_sentance\"] = train['text'].apply(one_word_sentance)\n",
    "test[\"one_word_sentance\"] = test['text'].apply(one_word_sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.apply(get_sentiment_score, axis=1)\n",
    "# test = test.apply(get_sentiment_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[\"emotional_devergence_score\"]=(train[\"positive_score\"]-train[\"negative_score\"])/10.0\n",
    "# test[\"emotional_devergence_score\"]=(test[\"positive_score\"]-test[\"negative_score\"])/10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Informativeness</th>\n",
       "      <th>text</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>emotional_devergence_score</th>\n",
       "      <th>author_id</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>has_description</th>\n",
       "      <th>bio_has_url</th>\n",
       "      <th>...</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>hashtag_present</th>\n",
       "      <th>link_present</th>\n",
       "      <th>emoji_present</th>\n",
       "      <th>instructinal_keyword</th>\n",
       "      <th>contains_phone_number</th>\n",
       "      <th>slang_present</th>\n",
       "      <th>is_RT</th>\n",
       "      <th>has_profanity</th>\n",
       "      <th>one_word_sentance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241490299215634434</td>\n",
       "      <td>1</td>\n",
       "      <td>Official death toll from #covid19 in the Unite...</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>449864075</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Irish &amp; European. Push button interests: #Brex...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1245916400981381130</td>\n",
       "      <td>1</td>\n",
       "      <td>Dearest Mr. President @USER 1,169 coronavirus ...</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>724994029281116160</td>\n",
       "      <td>tweet</td>\n",
       "      <td>pemerhati sosial dan bersahabat dengan kemanus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1241132432402849793</td>\n",
       "      <td>1</td>\n",
       "      <td>Latest Updates March 20 ⚠️5274 new cases and 3...</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1190410214954151936</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Fuck every name that ever existed.  Uses https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1241170177997357057</td>\n",
       "      <td>1</td>\n",
       "      <td>Latest Updates March 21 ⚠️5725 new cases and 5...</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1190410214954151936</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Fuck every name that ever existed.  Uses https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1241782965476212737</td>\n",
       "      <td>1</td>\n",
       "      <td>Latest Updates March 22 ➡️5560 new cases and 6...</td>\n",
       "      <td>3</td>\n",
       "      <td>-3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1190410214954151936</td>\n",
       "      <td>tweet</td>\n",
       "      <td>Fuck every name that ever existed.  Uses https...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  Informativeness  \\\n",
       "0  1241490299215634434                1   \n",
       "1  1245916400981381130                1   \n",
       "2  1241132432402849793                1   \n",
       "3  1241170177997357057                1   \n",
       "4  1241782965476212737                1   \n",
       "\n",
       "                                                text  positive_score  \\\n",
       "0  Official death toll from #covid19 in the Unite...               3   \n",
       "1  Dearest Mr. President @USER 1,169 coronavirus ...               3   \n",
       "2  Latest Updates March 20 ⚠️5274 new cases and 3...               1   \n",
       "3  Latest Updates March 21 ⚠️5725 new cases and 5...               1   \n",
       "4  Latest Updates March 22 ➡️5560 new cases and 6...               3   \n",
       "\n",
       "   negative_score  emotional_devergence_score            author_id tweet_type  \\\n",
       "0              -3                         0.6            449864075      tweet   \n",
       "1              -3                         0.6   724994029281116160      tweet   \n",
       "2              -3                         0.4  1190410214954151936      tweet   \n",
       "3              -3                         0.4  1190410214954151936      tweet   \n",
       "4              -3                         0.6  1190410214954151936      tweet   \n",
       "\n",
       "                                     has_description bio_has_url  ...  \\\n",
       "0  Irish & European. Push button interests: #Brex...         NaN  ...   \n",
       "1  pemerhati sosial dan bersahabat dengan kemanus...         NaN  ...   \n",
       "2  Fuck every name that ever existed.  Uses https...         NaN  ...   \n",
       "3  Fuck every name that ever existed.  Uses https...         NaN  ...   \n",
       "4  Fuck every name that ever existed.  Uses https...         NaN  ...   \n",
       "\n",
       "   hashtag_count  hashtag_present  link_present  emoji_present  \\\n",
       "0              1                1             0          False   \n",
       "1              1                1             0          False   \n",
       "2              0                0             0          False   \n",
       "3              0                0             0          False   \n",
       "4              0                0             0          False   \n",
       "\n",
       "   instructinal_keyword  contains_phone_number  slang_present  is_RT  \\\n",
       "0                 False                  False          False  False   \n",
       "1                 False                  False          False  False   \n",
       "2                 False                  False          False  False   \n",
       "3                 False                  False          False  False   \n",
       "4                 False                  False          False  False   \n",
       "\n",
       "   has_profanity  one_word_sentance  \n",
       "0          False                  0  \n",
       "1          False                  0  \n",
       "2          False                  0  \n",
       "3          False                  0  \n",
       "4          False                  0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'author_id',\n",
       " 'tweet_type',\n",
       " 'has_description',\n",
       " 'bio_has_url',\n",
       " 'protected',\n",
       " 'default_profile',\n",
       " 'default_profile_image']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['id',  'author_id', 'tweet_type','has_description', 'bio_has_url','protected', 'default_profile','default_profile_image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(columns=['id',  'author_id', 'tweet_type','has_description', 'bio_has_url','protected', 'default_profile','default_profile_image'])\n",
    "test= test.drop(columns=['id',  'author_id', 'tweet_type','has_description', 'bio_has_url','protected', 'default_profile','default_profile_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 46025/46025 [02:54<00:00, 263.57it/s]\n",
      "100%|██████████| 6955/6955 [00:55<00:00, 125.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "train['text']=train['text'].progress_apply(text_processing)\n",
    "test['text']=test['text'].progress_apply(text_processing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(train, './word2vec_data/train.pkl')\n",
    "pd.to_pickle(test, './word2vec_data/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39774 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "texts=train.text\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences_train = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train : (46025, 26)\n",
      "Shape of label train : (46025,)\n"
     ]
    }
   ],
   "source": [
    "X_train = pad_sequences(sequences_train)\n",
    "y_train = np.asarray(train.Informativeness)\n",
    "print('Shape of X train :', X_train.shape)\n",
    "print('Shape of label train :', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(train.text.apply(lambda x: len(x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('./word2vec_data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "EMBEDDING_DIM=300\n",
    "vocabulary_size=len(word_index)+1\n",
    "embedding_matrix = np.zeros((vocabulary_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = word_vectors[word]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "del(word_vectors)\n",
    "\n",
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39775, 300)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46025, 21)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "sequence_length = X_train.shape[1]\n",
    "\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 150\n",
    "drop = 0.5\n",
    "\n",
    "\n",
    "\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "\n",
    "#CHANGE THIS!\n",
    "meta_input = Input(shape=(19,))\n",
    "embedding = embedding_layer(inputs)\n",
    "reshape = Reshape((sequence_length,EMBEDDING_DIM,1))(embedding)\n",
    "\n",
    "\n",
    "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM),activation='relu',kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "\n",
    "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
    "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
    "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
    "\n",
    "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
    "flatten = Flatten()(merged_tensor)\n",
    "reshape = Reshape((3*num_filters,))(flatten)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "merged_tensor2 = concatenate([dropout, meta_input])\n",
    "\n",
    "output = Dense(units=2, activation='softmax',kernel_regularizer=regularizers.l2(0.01))(merged_tensor2)\n",
    "\n",
    "\n",
    "\n",
    "model = Model([inputs , meta_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(len(y_train), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_extra = np.abs(y_train-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_meta = train.to_numpy()[:, 2:].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46025, 19)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "461/461 [==============================] - 162s 351ms/step - loss: 197.8209 - auc: 0.7874\n",
      "Epoch 2/10\n",
      "461/461 [==============================] - 173s 375ms/step - loss: 117.1993 - auc: 0.8244\n",
      "Epoch 3/10\n",
      "461/461 [==============================] - 138s 298ms/step - loss: 110.4707 - auc: 0.8512\n",
      "Epoch 4/10\n",
      "461/461 [==============================] - 151s 328ms/step - loss: 109.4006 - auc: 0.8657\n",
      "Epoch 5/10\n",
      "461/461 [==============================] - 150s 325ms/step - loss: 164.5964 - auc: 0.8767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b26940f90>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['AUC'])\n",
    "callbacks = [EarlyStopping(monitor='loss')]\n",
    "model.fit([X_train, X_train_meta], np.append(y_train, y_train_extra, axis=1), batch_size=100, epochs=10, verbose=1,\n",
    "         callbacks=callbacks)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test=tokenizer.texts_to_sequences(test.text)\n",
    "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n",
    "X_test_meta = test.to_numpy()[:, 2:].astype('float32')\n",
    "y_pred=model.predict([X_test, X_test_meta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bool =(y_pred > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.asarray(test.Informativeness).reshape(len(test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.5529834651329979, f1: 0.5879390324718357, roc: 0.5595209238886529\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"Acc: {}, f1: {}, roc: {}\".format(accuracy_score(y_true, y_pred_bool[:,0]),f1_score(y_true, y_pred_bool[:,0]),roc_auc_score(y_true, y_pred_bool[:,0])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
